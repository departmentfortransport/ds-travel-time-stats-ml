{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('05-transformed-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29541, 51)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_numbers = np.random.random(len(df))\n",
    "df_small = df[random_numbers < 0.05]\n",
    "target = df_small.travel_time\n",
    "features = StandardScaler().fit_transform(df_small.drop('travel_time', axis=1))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric\n",
    "Does the evaluation metric matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_mean_absolute_error score -3.64\n",
      "neg_mean_squared_error score -37.24\n",
      "neg_median_absolute_error score -1.97\n",
      "r2 score 0.41\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "for scorer in ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'r2']:\n",
    "    scores = cross_val_score(rf, features, target, scoring=scorer)\n",
    "    print(scorer + ' score ' + '{:.2f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_mean_absolute_error score -3.21\n",
      "neg_mean_squared_error score -30.54\n",
      "neg_median_absolute_error score -1.90\n",
      "r2 score 0.51\n"
     ]
    }
   ],
   "source": [
    "rf = GradientBoostingRegressor()\n",
    "for scorer in ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'r2']:\n",
    "    scores = cross_val_score(rf, features, target, scoring=scorer)\n",
    "    print(scorer + ' score ' + '{:.2f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking is the same across models for both metrics. Use median absolute error as it is easy to interpret. We would like this value to be less than 1 minute ideally.\n",
    "Lets see if the model can be tuned to reach that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "## Random forest\n",
    "### How many trees until performance drops off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_score(n_trees):\n",
    "    rf = RandomForestRegressor(n_estimators = n_trees, n_jobs=-1)\n",
    "    scores = cross_val_score(rf, features, target, scoring='neg_median_absolute_error')\n",
    "    return(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10ddca748>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cd11668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for n in [10, 50, 100, 150, 200, 250, 300, 350, 400, 450]:\n",
    "    results.append(random_forest_score(n))\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10eae6be0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e24a6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([10, 50, 100, 150, 200, 250, 300, 350, 400, 450], results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune random forest parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['mse'], # 'mae' was taking too long\n",
    "    'max_features': [50, 5, 10, 25],\n",
    "    'max_depth': [None, 3, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "cv_search = GridSearchCV(rf, param_grid,  \n",
    "                         scoring='neg_median_absolute_error', cv=3, \n",
    "                         verbose=1, error_score = -999 )\n",
    "cv_search = cv_search.fit(X=features, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.765471</td>\n",
       "      <td>0.107118</td>\n",
       "      <td>-1.850915</td>\n",
       "      <td>-1.766620</td>\n",
       "      <td>mse</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 5, 'max_feat...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.799286</td>\n",
       "      <td>-1.942304</td>\n",
       "      <td>-1.809461</td>\n",
       "      <td>-1.727410</td>\n",
       "      <td>-1.943999</td>\n",
       "      <td>-1.630147</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.065951</td>\n",
       "      <td>0.130419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.782820</td>\n",
       "      <td>0.107267</td>\n",
       "      <td>-1.888595</td>\n",
       "      <td>-1.475507</td>\n",
       "      <td>mse</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 10, 'max_fea...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.547367</td>\n",
       "      <td>-1.659347</td>\n",
       "      <td>-1.864825</td>\n",
       "      <td>-1.402510</td>\n",
       "      <td>-2.253592</td>\n",
       "      <td>-1.364663</td>\n",
       "      <td>0.249516</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.288804</td>\n",
       "      <td>0.130910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.879909</td>\n",
       "      <td>0.107797</td>\n",
       "      <td>-1.890894</td>\n",
       "      <td>-1.481643</td>\n",
       "      <td>mse</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 10, 'max_fea...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.546735</td>\n",
       "      <td>-1.675521</td>\n",
       "      <td>-1.858748</td>\n",
       "      <td>-1.406877</td>\n",
       "      <td>-2.267198</td>\n",
       "      <td>-1.362531</td>\n",
       "      <td>0.127467</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.295005</td>\n",
       "      <td>0.138282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.950799</td>\n",
       "      <td>0.108621</td>\n",
       "      <td>-1.951217</td>\n",
       "      <td>-1.797863</td>\n",
       "      <td>mse</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 5, 'max_feat...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.868089</td>\n",
       "      <td>-1.993867</td>\n",
       "      <td>-1.872770</td>\n",
       "      <td>-1.754391</td>\n",
       "      <td>-2.112793</td>\n",
       "      <td>-1.645332</td>\n",
       "      <td>0.049425</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.114268</td>\n",
       "      <td>0.145571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.888205</td>\n",
       "      <td>0.107063</td>\n",
       "      <td>-1.991326</td>\n",
       "      <td>-1.533858</td>\n",
       "      <td>mse</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 10, 'max_fea...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.594746</td>\n",
       "      <td>-1.723815</td>\n",
       "      <td>-1.967359</td>\n",
       "      <td>-1.457329</td>\n",
       "      <td>-2.411872</td>\n",
       "      <td>-1.420431</td>\n",
       "      <td>0.048753</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.334020</td>\n",
       "      <td>0.135162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.612387</td>\n",
       "      <td>0.110017</td>\n",
       "      <td>-2.002546</td>\n",
       "      <td>-0.584341</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': None, 'max_f...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.590809</td>\n",
       "      <td>-0.655754</td>\n",
       "      <td>-1.970000</td>\n",
       "      <td>-0.560000</td>\n",
       "      <td>-2.446829</td>\n",
       "      <td>-0.537269</td>\n",
       "      <td>0.133125</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.051342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.587474</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>-2.024875</td>\n",
       "      <td>-0.589359</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': None, 'max_f...</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.648215</td>\n",
       "      <td>-0.661686</td>\n",
       "      <td>-1.962099</td>\n",
       "      <td>-0.566392</td>\n",
       "      <td>-2.464310</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>0.259619</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.336113</td>\n",
       "      <td>0.052265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.557852</td>\n",
       "      <td>0.108071</td>\n",
       "      <td>-2.059759</td>\n",
       "      <td>-0.591469</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': None, 'max_f...</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.591012</td>\n",
       "      <td>-0.668312</td>\n",
       "      <td>-2.059327</td>\n",
       "      <td>-0.562306</td>\n",
       "      <td>-2.528940</td>\n",
       "      <td>-0.543790</td>\n",
       "      <td>0.044819</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.382908</td>\n",
       "      <td>0.054859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.122202</td>\n",
       "      <td>0.107615</td>\n",
       "      <td>-2.169674</td>\n",
       "      <td>-2.069095</td>\n",
       "      <td>mse</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 3, 'max_feat...</td>\n",
       "      <td>9</td>\n",
       "      <td>-2.278559</td>\n",
       "      <td>-2.284152</td>\n",
       "      <td>-1.995613</td>\n",
       "      <td>-1.991222</td>\n",
       "      <td>-2.234850</td>\n",
       "      <td>-1.931912</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.124366</td>\n",
       "      <td>0.153984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915073</td>\n",
       "      <td>0.109043</td>\n",
       "      <td>-2.308298</td>\n",
       "      <td>-0.625902</td>\n",
       "      <td>mse</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': None, 'max_f...</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.661575</td>\n",
       "      <td>-0.713096</td>\n",
       "      <td>-2.286305</td>\n",
       "      <td>-0.592518</td>\n",
       "      <td>-2.977014</td>\n",
       "      <td>-0.572093</td>\n",
       "      <td>0.005605</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.537251</td>\n",
       "      <td>0.062216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.502756</td>\n",
       "      <td>0.108580</td>\n",
       "      <td>-2.310971</td>\n",
       "      <td>-2.084213</td>\n",
       "      <td>mse</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 5, 'max_feat...</td>\n",
       "      <td>11</td>\n",
       "      <td>-2.057977</td>\n",
       "      <td>-2.408822</td>\n",
       "      <td>-2.218628</td>\n",
       "      <td>-1.980490</td>\n",
       "      <td>-2.656309</td>\n",
       "      <td>-1.863329</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.252845</td>\n",
       "      <td>0.234463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.640764</td>\n",
       "      <td>0.108810</td>\n",
       "      <td>-2.360189</td>\n",
       "      <td>-2.232284</td>\n",
       "      <td>mse</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 3, 'max_feat...</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.477975</td>\n",
       "      <td>-2.472571</td>\n",
       "      <td>-2.175558</td>\n",
       "      <td>-2.165634</td>\n",
       "      <td>-2.427034</td>\n",
       "      <td>-2.058647</td>\n",
       "      <td>0.046332</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.175432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.508858</td>\n",
       "      <td>0.107515</td>\n",
       "      <td>-2.464774</td>\n",
       "      <td>-1.757896</td>\n",
       "      <td>mse</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 10, 'max_fea...</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.725922</td>\n",
       "      <td>-1.979567</td>\n",
       "      <td>-2.325821</td>\n",
       "      <td>-1.665431</td>\n",
       "      <td>-3.342580</td>\n",
       "      <td>-1.628691</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.667271</td>\n",
       "      <td>0.157461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.306267</td>\n",
       "      <td>0.106028</td>\n",
       "      <td>-2.908730</td>\n",
       "      <td>-2.522518</td>\n",
       "      <td>mse</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 5, 'max_feat...</td>\n",
       "      <td>14</td>\n",
       "      <td>-2.456523</td>\n",
       "      <td>-2.957828</td>\n",
       "      <td>-2.809031</td>\n",
       "      <td>-2.404987</td>\n",
       "      <td>-3.460635</td>\n",
       "      <td>-2.204740</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.415945</td>\n",
       "      <td>0.318481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.106649</td>\n",
       "      <td>-2.909399</td>\n",
       "      <td>-2.630099</td>\n",
       "      <td>mse</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 3, 'max_feat...</td>\n",
       "      <td>15</td>\n",
       "      <td>-2.972220</td>\n",
       "      <td>-3.011353</td>\n",
       "      <td>-2.667777</td>\n",
       "      <td>-2.488041</td>\n",
       "      <td>-3.088198</td>\n",
       "      <td>-2.390903</td>\n",
       "      <td>0.046873</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.177292</td>\n",
       "      <td>0.272488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.295659</td>\n",
       "      <td>0.108467</td>\n",
       "      <td>-3.269465</td>\n",
       "      <td>-2.969385</td>\n",
       "      <td>mse</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 3, 'max_feat...</td>\n",
       "      <td>16</td>\n",
       "      <td>-3.091083</td>\n",
       "      <td>-3.447791</td>\n",
       "      <td>-3.136145</td>\n",
       "      <td>-2.822559</td>\n",
       "      <td>-3.581168</td>\n",
       "      <td>-2.637804</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.221174</td>\n",
       "      <td>0.346591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "8        1.765471         0.107118        -1.850915         -1.766620   \n",
       "12       3.782820         0.107267        -1.888595         -1.475507   \n",
       "15       1.879909         0.107797        -1.890894         -1.481643   \n",
       "11       0.950799         0.108621        -1.951217         -1.797863   \n",
       "14       0.888205         0.107063        -1.991326         -1.533858   \n",
       "3        3.612387         0.110017        -2.002546         -0.584341   \n",
       "0        6.587474         0.108247        -2.024875         -0.589359   \n",
       "2        1.557852         0.108071        -2.059759         -0.591469   \n",
       "4        1.122202         0.107615        -2.169674         -2.069095   \n",
       "1        0.915073         0.109043        -2.308298         -0.625902   \n",
       "10       0.502756         0.108580        -2.310971         -2.084213   \n",
       "7        0.640764         0.108810        -2.360189         -2.232284   \n",
       "13       0.508858         0.107515        -2.464774         -1.757896   \n",
       "9        0.306267         0.106028        -2.908730         -2.522518   \n",
       "6        0.366889         0.106649        -2.909399         -2.630099   \n",
       "5        0.295659         0.108467        -3.269465         -2.969385   \n",
       "\n",
       "   param_criterion param_max_depth param_max_features  \\\n",
       "8              mse               5                 50   \n",
       "12             mse              10                 50   \n",
       "15             mse              10                 25   \n",
       "11             mse               5                 25   \n",
       "14             mse              10                 10   \n",
       "3              mse            None                 25   \n",
       "0              mse            None                 50   \n",
       "2              mse            None                 10   \n",
       "4              mse               3                 50   \n",
       "1              mse            None                  5   \n",
       "10             mse               5                 10   \n",
       "7              mse               3                 25   \n",
       "13             mse              10                  5   \n",
       "9              mse               5                  5   \n",
       "6              mse               3                 10   \n",
       "5              mse               3                  5   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "8   {'criterion': 'mse', 'max_depth': 5, 'max_feat...                1   \n",
       "12  {'criterion': 'mse', 'max_depth': 10, 'max_fea...                2   \n",
       "15  {'criterion': 'mse', 'max_depth': 10, 'max_fea...                3   \n",
       "11  {'criterion': 'mse', 'max_depth': 5, 'max_feat...                4   \n",
       "14  {'criterion': 'mse', 'max_depth': 10, 'max_fea...                5   \n",
       "3   {'criterion': 'mse', 'max_depth': None, 'max_f...                6   \n",
       "0   {'criterion': 'mse', 'max_depth': None, 'max_f...                7   \n",
       "2   {'criterion': 'mse', 'max_depth': None, 'max_f...                8   \n",
       "4   {'criterion': 'mse', 'max_depth': 3, 'max_feat...                9   \n",
       "1   {'criterion': 'mse', 'max_depth': None, 'max_f...               10   \n",
       "10  {'criterion': 'mse', 'max_depth': 5, 'max_feat...               11   \n",
       "7   {'criterion': 'mse', 'max_depth': 3, 'max_feat...               12   \n",
       "13  {'criterion': 'mse', 'max_depth': 10, 'max_fea...               13   \n",
       "9   {'criterion': 'mse', 'max_depth': 5, 'max_feat...               14   \n",
       "6   {'criterion': 'mse', 'max_depth': 3, 'max_feat...               15   \n",
       "5   {'criterion': 'mse', 'max_depth': 3, 'max_feat...               16   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "8           -1.799286           -1.942304          -1.809461   \n",
       "12          -1.547367           -1.659347          -1.864825   \n",
       "15          -1.546735           -1.675521          -1.858748   \n",
       "11          -1.868089           -1.993867          -1.872770   \n",
       "14          -1.594746           -1.723815          -1.967359   \n",
       "3           -1.590809           -0.655754          -1.970000   \n",
       "0           -1.648215           -0.661686          -1.962099   \n",
       "2           -1.591012           -0.668312          -2.059327   \n",
       "4           -2.278559           -2.284152          -1.995613   \n",
       "1           -1.661575           -0.713096          -2.286305   \n",
       "10          -2.057977           -2.408822          -2.218628   \n",
       "7           -2.477975           -2.472571          -2.175558   \n",
       "13          -1.725922           -1.979567          -2.325821   \n",
       "9           -2.456523           -2.957828          -2.809031   \n",
       "6           -2.972220           -3.011353          -2.667777   \n",
       "5           -3.091083           -3.447791          -3.136145   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "8            -1.727410          -1.943999           -1.630147      0.045124   \n",
       "12           -1.402510          -2.253592           -1.364663      0.249516   \n",
       "15           -1.406877          -2.267198           -1.362531      0.127467   \n",
       "11           -1.754391          -2.112793           -1.645332      0.049425   \n",
       "14           -1.457329          -2.411872           -1.420431      0.048753   \n",
       "3            -0.560000          -2.446829           -0.537269      0.133125   \n",
       "0            -0.566392          -2.464310           -0.540000      0.259619   \n",
       "2            -0.562306          -2.528940           -0.543790      0.044819   \n",
       "4            -1.991222          -2.234850           -1.931912      0.004648   \n",
       "1            -0.592518          -2.977014           -0.572093      0.005605   \n",
       "10           -1.980490          -2.656309           -1.863329      0.003639   \n",
       "7            -2.165634          -2.427034           -2.058647      0.046332   \n",
       "13           -1.665431          -3.342580           -1.628691      0.001159   \n",
       "9            -2.404987          -3.460635           -2.204740      0.005606   \n",
       "6            -2.488041          -3.088198           -2.390903      0.046873   \n",
       "5            -2.822559          -3.581168           -2.637804      0.003012   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "8         0.000761        0.065951         0.130419  \n",
       "12        0.001997        0.288804         0.130910  \n",
       "15        0.001160        0.295005         0.138282  \n",
       "11        0.001112        0.114268         0.145571  \n",
       "14        0.000641        0.334020         0.135162  \n",
       "3         0.000828        0.350225         0.051342  \n",
       "0         0.002130        0.336113         0.052265  \n",
       "2         0.001159        0.382908         0.054859  \n",
       "4         0.001746        0.124366         0.153984  \n",
       "1         0.001185        0.537251         0.062216  \n",
       "10        0.002012        0.252845         0.234463  \n",
       "7         0.000763        0.132200         0.175432  \n",
       "13        0.001296        0.667271         0.157461  \n",
       "9         0.000471        0.415945         0.318481  \n",
       "6         0.000294        0.177292         0.272488  \n",
       "5         0.001899        0.221174         0.346591  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.DataFrame(cv_search.cv_results_)\n",
    "search_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.078628</td>\n",
       "      <td>0.108147</td>\n",
       "      <td>-1.861791</td>\n",
       "      <td>-1.703909</td>\n",
       "      <td>mse</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 6, 'max_feat...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.720439</td>\n",
       "      <td>-1.895335</td>\n",
       "      <td>-1.819191</td>\n",
       "      <td>-1.634394</td>\n",
       "      <td>-2.045745</td>\n",
       "      <td>-1.581997</td>\n",
       "      <td>0.047801</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.136179</td>\n",
       "      <td>0.137039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.792203</td>\n",
       "      <td>0.108683</td>\n",
       "      <td>-1.865271</td>\n",
       "      <td>-1.596944</td>\n",
       "      <td>mse</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 8, 'max_feat...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.595935</td>\n",
       "      <td>-1.796313</td>\n",
       "      <td>-1.826804</td>\n",
       "      <td>-1.524513</td>\n",
       "      <td>-2.173075</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>0.054606</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.237181</td>\n",
       "      <td>0.142720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.894453</td>\n",
       "      <td>0.105878</td>\n",
       "      <td>-1.919738</td>\n",
       "      <td>-1.471361</td>\n",
       "      <td>mse</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 10, 'max_fea...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.534065</td>\n",
       "      <td>-1.663460</td>\n",
       "      <td>-1.886182</td>\n",
       "      <td>-1.389443</td>\n",
       "      <td>-2.338967</td>\n",
       "      <td>-1.361181</td>\n",
       "      <td>0.237746</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.329455</td>\n",
       "      <td>0.136323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.510031</td>\n",
       "      <td>0.108633</td>\n",
       "      <td>-1.930444</td>\n",
       "      <td>-1.316931</td>\n",
       "      <td>mse</td>\n",
       "      <td>12</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 12, 'max_fea...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.545043</td>\n",
       "      <td>-1.477616</td>\n",
       "      <td>-1.902525</td>\n",
       "      <td>-1.244916</td>\n",
       "      <td>-2.343765</td>\n",
       "      <td>-1.228261</td>\n",
       "      <td>0.291267</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.326674</td>\n",
       "      <td>0.113824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.422994</td>\n",
       "      <td>0.109403</td>\n",
       "      <td>-1.947429</td>\n",
       "      <td>-1.903207</td>\n",
       "      <td>mse</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 4, 'max_feat...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.988411</td>\n",
       "      <td>-2.131844</td>\n",
       "      <td>-1.884776</td>\n",
       "      <td>-1.842069</td>\n",
       "      <td>-1.969100</td>\n",
       "      <td>-1.735708</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>0.167401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.202246</td>\n",
       "      <td>0.104206</td>\n",
       "      <td>-1.982379</td>\n",
       "      <td>-1.155396</td>\n",
       "      <td>mse</td>\n",
       "      <td>14</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 14, 'max_fea...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.591274</td>\n",
       "      <td>-1.303665</td>\n",
       "      <td>-1.944508</td>\n",
       "      <td>-1.079833</td>\n",
       "      <td>-2.411354</td>\n",
       "      <td>-1.082690</td>\n",
       "      <td>0.147357</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.335866</td>\n",
       "      <td>0.104848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.601079</td>\n",
       "      <td>0.108334</td>\n",
       "      <td>-2.011772</td>\n",
       "      <td>-1.005291</td>\n",
       "      <td>mse</td>\n",
       "      <td>16</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'criterion': 'mse', 'max_depth': 16, 'max_fea...</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.606769</td>\n",
       "      <td>-1.146702</td>\n",
       "      <td>-1.979769</td>\n",
       "      <td>-0.924086</td>\n",
       "      <td>-2.448778</td>\n",
       "      <td>-0.945086</td>\n",
       "      <td>0.312384</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.344493</td>\n",
       "      <td>0.100359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "1       2.078628         0.108147        -1.861791         -1.703909   \n",
       "2       2.792203         0.108683        -1.865271         -1.596944   \n",
       "3       3.894453         0.105878        -1.919738         -1.471361   \n",
       "4       4.510031         0.108633        -1.930444         -1.316931   \n",
       "0       1.422994         0.109403        -1.947429         -1.903207   \n",
       "5       5.202246         0.104206        -1.982379         -1.155396   \n",
       "6       5.601079         0.108334        -2.011772         -1.005291   \n",
       "\n",
       "  param_criterion param_max_depth param_max_features  \\\n",
       "1             mse               6               auto   \n",
       "2             mse               8               auto   \n",
       "3             mse              10               auto   \n",
       "4             mse              12               auto   \n",
       "0             mse               4               auto   \n",
       "5             mse              14               auto   \n",
       "6             mse              16               auto   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "1  {'criterion': 'mse', 'max_depth': 6, 'max_feat...                1   \n",
       "2  {'criterion': 'mse', 'max_depth': 8, 'max_feat...                2   \n",
       "3  {'criterion': 'mse', 'max_depth': 10, 'max_fea...                3   \n",
       "4  {'criterion': 'mse', 'max_depth': 12, 'max_fea...                4   \n",
       "0  {'criterion': 'mse', 'max_depth': 4, 'max_feat...                5   \n",
       "5  {'criterion': 'mse', 'max_depth': 14, 'max_fea...                6   \n",
       "6  {'criterion': 'mse', 'max_depth': 16, 'max_fea...                7   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "1          -1.720439           -1.895335          -1.819191   \n",
       "2          -1.595935           -1.796313          -1.826804   \n",
       "3          -1.534065           -1.663460          -1.886182   \n",
       "4          -1.545043           -1.477616          -1.902525   \n",
       "0          -1.988411           -2.131844          -1.884776   \n",
       "5          -1.591274           -1.303665          -1.944508   \n",
       "6          -1.606769           -1.146702          -1.979769   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "1           -1.634394          -2.045745           -1.581997      0.047801   \n",
       "2           -1.524513          -2.173075           -1.470007      0.054606   \n",
       "3           -1.389443          -2.338967           -1.361181      0.237746   \n",
       "4           -1.244916          -2.343765           -1.228261      0.291267   \n",
       "0           -1.842069          -1.969100           -1.735708      0.001694   \n",
       "5           -1.079833          -2.411354           -1.082690      0.147357   \n",
       "6           -0.924086          -2.448778           -0.945086      0.312384   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "1        0.001611        0.136179         0.137039  \n",
       "2        0.001046        0.237181         0.142720  \n",
       "3        0.001436        0.329455         0.136323  \n",
       "4        0.000748        0.326674         0.113824  \n",
       "0        0.001323        0.044998         0.167401  \n",
       "5        0.000304        0.335866         0.104848  \n",
       "6        0.001434        0.344493         0.100359  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['mse'], # 'mae' was taking too long\n",
    "    'max_features': ['auto'],\n",
    "    'max_depth': [4, 6, 8, 10, 12, 14, 16]\n",
    "}\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "cv_search = GridSearchCV(rf, param_grid,  \n",
    "                         scoring='neg_median_absolute_error', cv=3, \n",
    "                         verbose=1, error_score = -999 )\n",
    "cv_search = cv_search.fit(X=features, y=target)\n",
    "search_results = pd.DataFrame(cv_search.cv_results_)\n",
    "search_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning reduced the median error from 1.97 minutes to 1.86 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Trees tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=3 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=3 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=3 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=5 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=5 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=5 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=7 ........................\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=3, total=  14.5s\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=3, total=  14.6s\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=3, total=  15.0s\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=5, total=  27.7s\n",
      "[CV] learning_rate=0.01, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=5, total=  27.9s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=3 .......................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=5, total=  28.5s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=3 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=3, total=  14.2s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=3 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=3, total=  14.0s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=5 .......................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=7, total=  49.9s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=5 .......................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=7, total=  50.6s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=5 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=3, total=  13.9s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=7 .......................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=7, total=  46.3s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=7 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=5, total=  27.4s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=7 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=5, total=  26.1s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=9 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=5, total=  25.9s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=9 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=7, total=  45.0s\n",
      "[CV] learning_rate=0.01, loss=lad, max_depth=9 .......................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=7, total=  42.4s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=3 .....................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=9, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=3 .....................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=9, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=3 .....................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=7, total=  41.8s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=5 .....................\n",
      "[CV] ......... learning_rate=0.01, loss=ls, max_depth=9, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=5 .....................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=3, total=  14.7s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=5 .....................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=3, total=  14.6s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=7 .....................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=3, total=  14.6s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=7 .....................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=5, total=  27.9s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=7 .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=5, total=  27.2s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=9 .....................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=5, total=  26.6s\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=9 .....................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.01, loss=huber, max_depth=9 .....................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=3 ........................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=7, total=  46.8s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=3 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=3, total=  11.0s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=3 ........................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=7, total=  48.5s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=5 ........................\n",
      "[CV] ........ learning_rate=0.01, loss=lad, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=3, total=  12.0s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=3, total=  11.5s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=7 ........................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=7, total=  46.7s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=5, total=  23.7s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=5, total=  22.6s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=5, total=  22.4s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=7, total=  39.8s\n",
      "[CV] learning_rate=0.05, loss=ls, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=7, total=  39.0s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=3 .......................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=7, total=  37.3s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=3 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=3, total=  12.4s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=3 .......................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=9, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=5 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=3, total=  11.8s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=5 .......................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=9, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=5 .......................\n",
      "[CV] ...... learning_rate=0.01, loss=huber, max_depth=9, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=7 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=3, total=  13.2s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=7 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=5, total=  26.9s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=7 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=5, total=  26.1s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=9 .......................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=5, total=  26.4s\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=9 .......................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.05, loss=lad, max_depth=9 .......................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=3 .....................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=7, total=  43.4s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=3 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=3, total=  13.1s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=3 .....................\n",
      "[CV] ......... learning_rate=0.05, loss=ls, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=5 .....................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=7, total=  42.5s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=5 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=3, total=  13.7s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=5 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=3, total=  13.7s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=7 .....................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=7, total=  43.8s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=7 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=5, total=  26.5s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=7 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=5, total=  27.2s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=9 .....................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=5, total=  26.4s\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=9 .....................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.05, loss=huber, max_depth=9 .....................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=3 .........................\n",
      "[CV] ........ learning_rate=0.05, loss=lad, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=3 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=3, total=  11.5s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=3 .........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=7, total=  47.8s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=5 .........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=7, total=  46.4s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=5 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=3, total=  11.9s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=5 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=3, total=  11.7s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=7 .........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=7, total=  46.8s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=5, total=  23.5s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=5, total=  23.3s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=5, total=  22.3s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=7, total=  38.6s\n",
      "[CV] learning_rate=0.1, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=7, total=  37.6s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=3 ........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=9, total= 1.5min\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=3 ........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=7, total=  35.8s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=3 ........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=3, total=  12.3s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=5 ........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=9, total= 1.5min\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=3, total=  12.6s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=5 ........................\n",
      "[CV] ...... learning_rate=0.05, loss=huber, max_depth=9, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=7 ........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=3, total=  12.7s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=5, total=  25.0s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=5, total=  24.4s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=5, total=  23.8s\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.1, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=7, total=  37.9s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=7, total=  37.6s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=3 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=3, total=  13.0s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=5 ......................\n",
      "[CV] .......... learning_rate=0.1, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=3, total=  13.4s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=3, total=  13.4s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=7 ......................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=7, total=  37.0s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=7 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=5, total=  25.7s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=7 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=5, total=  25.6s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=5, total=  25.2s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=9 ......................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=9, total=  58.8s\n",
      "[CV] learning_rate=0.1, loss=huber, max_depth=9 ......................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=9, total=  57.8s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=3 .........................\n",
      "[CV] ......... learning_rate=0.1, loss=lad, max_depth=9, total=  58.3s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=3 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=3, total=  11.6s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=3 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=3, total=  11.9s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=5 .........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=7, total=  44.6s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=5 .........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=7, total=  43.6s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=5 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=3, total=  11.5s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=7 .........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=7, total=  40.8s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=5, total=  22.3s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=5, total=  22.0s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=5, total=  21.9s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=7, total=  38.1s\n",
      "[CV] learning_rate=0.2, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=7, total=  37.3s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=3 ........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=3 ........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=3 ........................\n",
      "[CV] ....... learning_rate=0.1, loss=huber, max_depth=9, total= 1.3min\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=5 ........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=7, total=  35.5s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=3, total=  12.3s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=3, total=  12.6s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=3, total=  12.8s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=5, total=  23.2s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=5, total=  22.9s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=5, total=  22.4s\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.2, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=9, total= 1.1min\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=7, total=  34.8s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=7, total=  33.0s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=3 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=3, total=  13.6s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=5 ......................\n",
      "[CV] .......... learning_rate=0.2, loss=ls, max_depth=9, total= 1.1min\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=5 ......................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=7, total=  34.6s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=3, total=  14.7s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=7 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=3, total=  14.6s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=7 ......................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=9, total=  52.5s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=7 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=5, total=  28.0s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=9 ......................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=9, total=  51.8s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=5, total=  26.6s\n",
      "[CV] learning_rate=0.2, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=5, total=  25.6s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=3 .........................\n",
      "[CV] ......... learning_rate=0.2, loss=lad, max_depth=9, total=  54.3s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=3 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=3, total=  12.3s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=3 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=3, total=  12.1s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=5 .........................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=7, total=  44.2s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=5 .........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=7, total=  44.1s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=5 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=3, total=  12.2s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=7 .........................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=7, total=  41.3s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=5, total=  23.7s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=7 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=5, total=  23.2s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=9 .........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 11.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=5, total=  24.0s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=9 .........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=7, total=  39.2s\n",
      "[CV] learning_rate=0.5, loss=ls, max_depth=9 .........................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=3 ........................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=3 ........................\n",
      "[CV] ....... learning_rate=0.2, loss=huber, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=3 ........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=7, total=  38.4s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=5 ........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=7, total=  37.5s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=3, total=  13.2s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=5 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=3, total=  13.3s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=3, total=  13.1s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=5, total=  22.6s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=7 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=5, total=  23.0s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=9 ........................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=5, total=  22.6s\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=9, total= 1.1min\n",
      "[CV] learning_rate=0.5, loss=lad, max_depth=9 ........................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=7, total=  33.7s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=7, total=  34.6s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=3 ......................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=7, total=  35.1s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=3, total=  15.6s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=3, total=  15.8s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=5 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=3, total=  15.8s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=7 ......................\n",
      "[CV] .......... learning_rate=0.5, loss=ls, max_depth=9, total= 1.2min\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=7 ......................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=9, total=  50.2s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=7 ......................\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=9, total=  49.0s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=5, total=  27.8s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=5, total=  27.2s\n",
      "[CV] learning_rate=0.5, loss=huber, max_depth=9 ......................\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=5, total=  26.7s\n",
      "[CV] ......... learning_rate=0.5, loss=lad, max_depth=9, total=  50.9s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=7, total=  39.6s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=7, total=  36.1s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=7, total=  28.7s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=9, total=  36.1s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=9, total=  42.1s\n",
      "[CV] ....... learning_rate=0.5, loss=huber, max_depth=9, total=  38.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 14.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.408733</td>\n",
       "      <td>0.042510</td>\n",
       "      <td>-1.727872</td>\n",
       "      <td>-1.621358</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.470123</td>\n",
       "      <td>-1.842604</td>\n",
       "      <td>-1.768484</td>\n",
       "      <td>-1.551389</td>\n",
       "      <td>-1.945008</td>\n",
       "      <td>-1.470083</td>\n",
       "      <td>0.562040</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.195986</td>\n",
       "      <td>0.159926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26.386224</td>\n",
       "      <td>0.068124</td>\n",
       "      <td>-1.731935</td>\n",
       "      <td>-1.482099</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.513285</td>\n",
       "      <td>-1.651843</td>\n",
       "      <td>-1.756297</td>\n",
       "      <td>-1.430296</td>\n",
       "      <td>-1.926222</td>\n",
       "      <td>-1.364159</td>\n",
       "      <td>0.340439</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.169459</td>\n",
       "      <td>0.123026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26.615493</td>\n",
       "      <td>0.068937</td>\n",
       "      <td>-1.791853</td>\n",
       "      <td>-1.558861</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.524351</td>\n",
       "      <td>-1.743231</td>\n",
       "      <td>-1.853912</td>\n",
       "      <td>-1.502879</td>\n",
       "      <td>-1.997297</td>\n",
       "      <td>-1.430474</td>\n",
       "      <td>0.381171</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.198003</td>\n",
       "      <td>0.133678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.447075</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>-1.803263</td>\n",
       "      <td>-1.674368</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.590906</td>\n",
       "      <td>-1.871494</td>\n",
       "      <td>-1.803082</td>\n",
       "      <td>-1.607305</td>\n",
       "      <td>-2.015801</td>\n",
       "      <td>-1.544305</td>\n",
       "      <td>0.313712</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.173462</td>\n",
       "      <td>0.141742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.138781</td>\n",
       "      <td>0.077171</td>\n",
       "      <td>-1.814402</td>\n",
       "      <td>-1.307753</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.730167</td>\n",
       "      <td>-1.446984</td>\n",
       "      <td>-1.748325</td>\n",
       "      <td>-1.257051</td>\n",
       "      <td>-1.964714</td>\n",
       "      <td>-1.219225</td>\n",
       "      <td>0.564184</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.106545</td>\n",
       "      <td>0.099655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>71.895877</td>\n",
       "      <td>0.086622</td>\n",
       "      <td>-1.841387</td>\n",
       "      <td>-1.099265</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.801479</td>\n",
       "      <td>-1.209665</td>\n",
       "      <td>-1.779027</td>\n",
       "      <td>-1.044392</td>\n",
       "      <td>-1.943655</td>\n",
       "      <td>-1.043736</td>\n",
       "      <td>0.833252</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.078066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.452810</td>\n",
       "      <td>0.058001</td>\n",
       "      <td>-1.842384</td>\n",
       "      <td>-1.583146</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.462753</td>\n",
       "      <td>-1.759570</td>\n",
       "      <td>-1.799038</td>\n",
       "      <td>-1.531443</td>\n",
       "      <td>-2.265361</td>\n",
       "      <td>-1.458426</td>\n",
       "      <td>0.168182</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.329093</td>\n",
       "      <td>0.128262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.987909</td>\n",
       "      <td>0.078316</td>\n",
       "      <td>-1.859638</td>\n",
       "      <td>-1.696569</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.789523</td>\n",
       "      <td>-1.886022</td>\n",
       "      <td>-1.826049</td>\n",
       "      <td>-1.626721</td>\n",
       "      <td>-1.963342</td>\n",
       "      <td>-1.576964</td>\n",
       "      <td>1.424292</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.074831</td>\n",
       "      <td>0.135495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.389244</td>\n",
       "      <td>0.060126</td>\n",
       "      <td>-1.861144</td>\n",
       "      <td>-1.748419</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.841710</td>\n",
       "      <td>-1.941143</td>\n",
       "      <td>-1.811980</td>\n",
       "      <td>-1.675511</td>\n",
       "      <td>-1.929742</td>\n",
       "      <td>-1.628602</td>\n",
       "      <td>0.691079</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.137616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.258625</td>\n",
       "      <td>0.094043</td>\n",
       "      <td>-1.897742</td>\n",
       "      <td>-1.749878</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.814639</td>\n",
       "      <td>-1.954869</td>\n",
       "      <td>-1.891613</td>\n",
       "      <td>-1.671992</td>\n",
       "      <td>-1.986974</td>\n",
       "      <td>-1.622775</td>\n",
       "      <td>0.847066</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.146336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77.393106</td>\n",
       "      <td>0.096048</td>\n",
       "      <td>-1.905636</td>\n",
       "      <td>-1.584666</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.825757</td>\n",
       "      <td>-1.758809</td>\n",
       "      <td>-1.872378</td>\n",
       "      <td>-1.508971</td>\n",
       "      <td>-2.018774</td>\n",
       "      <td>-1.486219</td>\n",
       "      <td>1.495793</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.082233</td>\n",
       "      <td>0.123488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.154617</td>\n",
       "      <td>0.062160</td>\n",
       "      <td>-1.929007</td>\n",
       "      <td>-1.814861</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.901725</td>\n",
       "      <td>-2.015583</td>\n",
       "      <td>-1.909856</td>\n",
       "      <td>-1.742419</td>\n",
       "      <td>-1.975440</td>\n",
       "      <td>-1.686583</td>\n",
       "      <td>0.505933</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>37.431016</td>\n",
       "      <td>0.071517</td>\n",
       "      <td>-1.929638</td>\n",
       "      <td>-1.253468</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.932853</td>\n",
       "      <td>-1.389050</td>\n",
       "      <td>-1.781885</td>\n",
       "      <td>-1.195953</td>\n",
       "      <td>-2.074177</td>\n",
       "      <td>-1.175401</td>\n",
       "      <td>0.363565</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>0.119349</td>\n",
       "      <td>0.096237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>95.674379</td>\n",
       "      <td>0.104283</td>\n",
       "      <td>-1.930600</td>\n",
       "      <td>-1.664430</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.831218</td>\n",
       "      <td>-1.868343</td>\n",
       "      <td>-1.899867</td>\n",
       "      <td>-1.582423</td>\n",
       "      <td>-2.060716</td>\n",
       "      <td>-1.542524</td>\n",
       "      <td>1.804403</td>\n",
       "      <td>0.011514</td>\n",
       "      <td>0.096179</td>\n",
       "      <td>0.145106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.981733</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>-1.943120</td>\n",
       "      <td>-1.822246</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>15</td>\n",
       "      <td>-2.037378</td>\n",
       "      <td>-2.049960</td>\n",
       "      <td>-1.817407</td>\n",
       "      <td>-1.703278</td>\n",
       "      <td>-1.974575</td>\n",
       "      <td>-1.713500</td>\n",
       "      <td>0.151283</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>0.161072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.851825</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>-1.954359</td>\n",
       "      <td>-1.621262</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.617713</td>\n",
       "      <td>-1.819882</td>\n",
       "      <td>-1.878392</td>\n",
       "      <td>-1.554622</td>\n",
       "      <td>-2.366973</td>\n",
       "      <td>-1.489281</td>\n",
       "      <td>0.601397</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.310565</td>\n",
       "      <td>0.142956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>46.944064</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>-1.961946</td>\n",
       "      <td>-1.408171</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.948573</td>\n",
       "      <td>-1.572034</td>\n",
       "      <td>-1.808551</td>\n",
       "      <td>-1.349839</td>\n",
       "      <td>-2.128712</td>\n",
       "      <td>-1.302641</td>\n",
       "      <td>0.582564</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.131047</td>\n",
       "      <td>0.117460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>58.188299</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>-1.970898</td>\n",
       "      <td>-1.042686</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>18</td>\n",
       "      <td>-2.042239</td>\n",
       "      <td>-1.159061</td>\n",
       "      <td>-1.833949</td>\n",
       "      <td>-0.992898</td>\n",
       "      <td>-2.036506</td>\n",
       "      <td>-0.976100</td>\n",
       "      <td>0.412954</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.096866</td>\n",
       "      <td>0.082575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.426562</td>\n",
       "      <td>0.054537</td>\n",
       "      <td>-1.979528</td>\n",
       "      <td>-1.768113</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>19</td>\n",
       "      <td>-1.661228</td>\n",
       "      <td>-1.990412</td>\n",
       "      <td>-1.941462</td>\n",
       "      <td>-1.699707</td>\n",
       "      <td>-2.335895</td>\n",
       "      <td>-1.614222</td>\n",
       "      <td>0.407961</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.276744</td>\n",
       "      <td>0.161016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.336335</td>\n",
       "      <td>0.072711</td>\n",
       "      <td>-1.983091</td>\n",
       "      <td>-1.420266</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>20</td>\n",
       "      <td>-2.012590</td>\n",
       "      <td>-1.580498</td>\n",
       "      <td>-1.763511</td>\n",
       "      <td>-1.369703</td>\n",
       "      <td>-2.173171</td>\n",
       "      <td>-1.310598</td>\n",
       "      <td>0.468893</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.115842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13.205518</td>\n",
       "      <td>0.052211</td>\n",
       "      <td>-1.991937</td>\n",
       "      <td>-1.639996</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.775052</td>\n",
       "      <td>-1.839935</td>\n",
       "      <td>-1.812237</td>\n",
       "      <td>-1.563534</td>\n",
       "      <td>-2.388522</td>\n",
       "      <td>-1.516520</td>\n",
       "      <td>0.182999</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.280838</td>\n",
       "      <td>0.142675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.586561</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>-2.032009</td>\n",
       "      <td>-1.962405</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>22</td>\n",
       "      <td>-2.178538</td>\n",
       "      <td>-2.203643</td>\n",
       "      <td>-1.909795</td>\n",
       "      <td>-1.864175</td>\n",
       "      <td>-2.007694</td>\n",
       "      <td>-1.819397</td>\n",
       "      <td>0.050646</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.111053</td>\n",
       "      <td>0.171558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>87.774307</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>-2.074851</td>\n",
       "      <td>-1.148613</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.147608</td>\n",
       "      <td>-1.276495</td>\n",
       "      <td>-1.886568</td>\n",
       "      <td>-1.082055</td>\n",
       "      <td>-2.190377</td>\n",
       "      <td>-1.087287</td>\n",
       "      <td>1.572514</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.090452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>76.752856</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>-2.190242</td>\n",
       "      <td>-1.232725</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>24</td>\n",
       "      <td>-2.063649</td>\n",
       "      <td>-1.370648</td>\n",
       "      <td>-1.958671</td>\n",
       "      <td>-1.167216</td>\n",
       "      <td>-2.548407</td>\n",
       "      <td>-1.160312</td>\n",
       "      <td>0.985835</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>0.256861</td>\n",
       "      <td>0.097567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12.498520</td>\n",
       "      <td>0.057985</td>\n",
       "      <td>-2.269567</td>\n",
       "      <td>-1.541942</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>25</td>\n",
       "      <td>-1.579116</td>\n",
       "      <td>-1.714248</td>\n",
       "      <td>-1.789248</td>\n",
       "      <td>-1.489417</td>\n",
       "      <td>-3.440339</td>\n",
       "      <td>-1.422161</td>\n",
       "      <td>0.213543</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.832293</td>\n",
       "      <td>0.124894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>34.068051</td>\n",
       "      <td>0.084313</td>\n",
       "      <td>-2.354884</td>\n",
       "      <td>-1.202369</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>26</td>\n",
       "      <td>-2.878401</td>\n",
       "      <td>-1.312147</td>\n",
       "      <td>-1.856960</td>\n",
       "      <td>-1.162457</td>\n",
       "      <td>-2.329291</td>\n",
       "      <td>-1.132502</td>\n",
       "      <td>0.816304</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.417394</td>\n",
       "      <td>0.078582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.844235</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>-2.460232</td>\n",
       "      <td>-2.324003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>27</td>\n",
       "      <td>-2.653820</td>\n",
       "      <td>-2.689870</td>\n",
       "      <td>-2.331474</td>\n",
       "      <td>-2.199878</td>\n",
       "      <td>-2.395401</td>\n",
       "      <td>-2.082263</td>\n",
       "      <td>1.860523</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.139354</td>\n",
       "      <td>0.263125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.617735</td>\n",
       "      <td>0.058882</td>\n",
       "      <td>-2.474042</td>\n",
       "      <td>-1.728981</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>28</td>\n",
       "      <td>-1.647166</td>\n",
       "      <td>-1.938385</td>\n",
       "      <td>-2.000044</td>\n",
       "      <td>-1.657973</td>\n",
       "      <td>-3.774918</td>\n",
       "      <td>-1.590583</td>\n",
       "      <td>0.160408</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.931070</td>\n",
       "      <td>0.150606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.963250</td>\n",
       "      <td>0.116644</td>\n",
       "      <td>-2.482616</td>\n",
       "      <td>-2.241182</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>29</td>\n",
       "      <td>-2.643710</td>\n",
       "      <td>-2.590111</td>\n",
       "      <td>-2.346420</td>\n",
       "      <td>-2.115677</td>\n",
       "      <td>-2.457720</td>\n",
       "      <td>-2.017758</td>\n",
       "      <td>1.874916</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.122638</td>\n",
       "      <td>0.249947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.155516</td>\n",
       "      <td>0.056368</td>\n",
       "      <td>-2.493272</td>\n",
       "      <td>-1.505715</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>30</td>\n",
       "      <td>-2.864763</td>\n",
       "      <td>-1.694434</td>\n",
       "      <td>-1.930032</td>\n",
       "      <td>-1.430251</td>\n",
       "      <td>-2.685020</td>\n",
       "      <td>-1.392460</td>\n",
       "      <td>0.068936</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>0.404974</td>\n",
       "      <td>0.134334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.951357</td>\n",
       "      <td>0.064363</td>\n",
       "      <td>-2.496180</td>\n",
       "      <td>-2.407415</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>31</td>\n",
       "      <td>-2.780718</td>\n",
       "      <td>-2.770946</td>\n",
       "      <td>-2.362601</td>\n",
       "      <td>-2.293836</td>\n",
       "      <td>-2.345221</td>\n",
       "      <td>-2.157461</td>\n",
       "      <td>0.334876</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.201324</td>\n",
       "      <td>0.263016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.626225</td>\n",
       "      <td>0.064243</td>\n",
       "      <td>-2.511971</td>\n",
       "      <td>-1.476421</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>32</td>\n",
       "      <td>-1.859132</td>\n",
       "      <td>-1.656558</td>\n",
       "      <td>-1.948277</td>\n",
       "      <td>-1.409389</td>\n",
       "      <td>-3.728505</td>\n",
       "      <td>-1.363316</td>\n",
       "      <td>1.064254</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.860989</td>\n",
       "      <td>0.128757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>75.469110</td>\n",
       "      <td>0.095184</td>\n",
       "      <td>-2.528460</td>\n",
       "      <td>-0.970846</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>33</td>\n",
       "      <td>-2.656695</td>\n",
       "      <td>-1.076976</td>\n",
       "      <td>-2.069040</td>\n",
       "      <td>-0.907608</td>\n",
       "      <td>-2.859644</td>\n",
       "      <td>-0.927955</td>\n",
       "      <td>1.775050</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.335258</td>\n",
       "      <td>0.075504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>52.788463</td>\n",
       "      <td>0.074381</td>\n",
       "      <td>-2.530191</td>\n",
       "      <td>-1.002044</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>34</td>\n",
       "      <td>-2.131337</td>\n",
       "      <td>-1.115096</td>\n",
       "      <td>-1.869918</td>\n",
       "      <td>-0.967361</td>\n",
       "      <td>-3.589319</td>\n",
       "      <td>-0.923676</td>\n",
       "      <td>1.082908</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.756482</td>\n",
       "      <td>0.081905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>22.802124</td>\n",
       "      <td>0.066880</td>\n",
       "      <td>-2.553307</td>\n",
       "      <td>-1.379776</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>35</td>\n",
       "      <td>-2.004566</td>\n",
       "      <td>-1.536712</td>\n",
       "      <td>-1.843151</td>\n",
       "      <td>-1.319659</td>\n",
       "      <td>-3.812204</td>\n",
       "      <td>-1.282957</td>\n",
       "      <td>0.315904</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.892610</td>\n",
       "      <td>0.111977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>69.821680</td>\n",
       "      <td>0.085676</td>\n",
       "      <td>-2.744141</td>\n",
       "      <td>-1.057491</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.304364</td>\n",
       "      <td>-1.166990</td>\n",
       "      <td>-2.191175</td>\n",
       "      <td>-1.009453</td>\n",
       "      <td>-3.736884</td>\n",
       "      <td>-0.996029</td>\n",
       "      <td>0.458487</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.703495</td>\n",
       "      <td>0.077622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.609785</td>\n",
       "      <td>0.081833</td>\n",
       "      <td>-2.763489</td>\n",
       "      <td>-2.647035</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>37</td>\n",
       "      <td>-3.235000</td>\n",
       "      <td>-3.015445</td>\n",
       "      <td>-2.533918</td>\n",
       "      <td>-2.528554</td>\n",
       "      <td>-2.521550</td>\n",
       "      <td>-2.397105</td>\n",
       "      <td>0.221060</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.333447</td>\n",
       "      <td>0.265975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.651472</td>\n",
       "      <td>0.077128</td>\n",
       "      <td>-2.850759</td>\n",
       "      <td>-1.333952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>38</td>\n",
       "      <td>-2.217088</td>\n",
       "      <td>-1.526027</td>\n",
       "      <td>-1.922535</td>\n",
       "      <td>-1.246456</td>\n",
       "      <td>-4.412653</td>\n",
       "      <td>-1.229374</td>\n",
       "      <td>0.184523</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>1.110953</td>\n",
       "      <td>0.135996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>72.188706</td>\n",
       "      <td>0.079550</td>\n",
       "      <td>-3.122529</td>\n",
       "      <td>-0.720488</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>39</td>\n",
       "      <td>-2.630357</td>\n",
       "      <td>-0.777981</td>\n",
       "      <td>-2.218549</td>\n",
       "      <td>-0.685091</td>\n",
       "      <td>-4.518682</td>\n",
       "      <td>-0.698393</td>\n",
       "      <td>1.868461</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>1.001442</td>\n",
       "      <td>0.041014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>68.400009</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>-3.141489</td>\n",
       "      <td>-0.795040</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>40</td>\n",
       "      <td>-3.006337</td>\n",
       "      <td>-0.882903</td>\n",
       "      <td>-2.361843</td>\n",
       "      <td>-0.744874</td>\n",
       "      <td>-4.056287</td>\n",
       "      <td>-0.757343</td>\n",
       "      <td>0.508664</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>0.062337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>22.003787</td>\n",
       "      <td>0.061957</td>\n",
       "      <td>-3.181125</td>\n",
       "      <td>-1.526304</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>41</td>\n",
       "      <td>-2.595424</td>\n",
       "      <td>-1.728139</td>\n",
       "      <td>-2.305569</td>\n",
       "      <td>-1.457869</td>\n",
       "      <td>-4.642382</td>\n",
       "      <td>-1.392903</td>\n",
       "      <td>0.145569</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>1.040018</td>\n",
       "      <td>0.145163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>42.924056</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>-3.306884</td>\n",
       "      <td>-1.314360</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>42</td>\n",
       "      <td>-2.436873</td>\n",
       "      <td>-1.454265</td>\n",
       "      <td>-1.895583</td>\n",
       "      <td>-1.251570</td>\n",
       "      <td>-5.588196</td>\n",
       "      <td>-1.237246</td>\n",
       "      <td>1.595740</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.628197</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37.289499</td>\n",
       "      <td>0.072119</td>\n",
       "      <td>-3.337655</td>\n",
       "      <td>-1.371297</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>43</td>\n",
       "      <td>-2.548415</td>\n",
       "      <td>-1.525439</td>\n",
       "      <td>-2.017653</td>\n",
       "      <td>-1.307659</td>\n",
       "      <td>-5.446897</td>\n",
       "      <td>-1.280793</td>\n",
       "      <td>1.150061</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>1.507117</td>\n",
       "      <td>0.109545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11.609967</td>\n",
       "      <td>0.051833</td>\n",
       "      <td>-3.381959</td>\n",
       "      <td>-1.710943</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>44</td>\n",
       "      <td>-2.202419</td>\n",
       "      <td>-1.933081</td>\n",
       "      <td>-1.983976</td>\n",
       "      <td>-1.626177</td>\n",
       "      <td>-5.959483</td>\n",
       "      <td>-1.573571</td>\n",
       "      <td>0.201665</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>1.824765</td>\n",
       "      <td>0.158537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>34.392599</td>\n",
       "      <td>0.091458</td>\n",
       "      <td>-3.533821</td>\n",
       "      <td>-1.119011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>45</td>\n",
       "      <td>-2.984584</td>\n",
       "      <td>-1.252455</td>\n",
       "      <td>-2.236219</td>\n",
       "      <td>-1.080975</td>\n",
       "      <td>-5.380660</td>\n",
       "      <td>-1.023601</td>\n",
       "      <td>0.587914</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>1.341174</td>\n",
       "      <td>0.097223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>49.971275</td>\n",
       "      <td>0.078717</td>\n",
       "      <td>-3.750595</td>\n",
       "      <td>-0.913287</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>46</td>\n",
       "      <td>-2.130313</td>\n",
       "      <td>-1.015405</td>\n",
       "      <td>-2.233688</td>\n",
       "      <td>-0.910113</td>\n",
       "      <td>-6.887783</td>\n",
       "      <td>-0.814342</td>\n",
       "      <td>0.785924</td>\n",
       "      <td>0.009275</td>\n",
       "      <td>2.218728</td>\n",
       "      <td>0.082114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.702921</td>\n",
       "      <td>0.063135</td>\n",
       "      <td>-3.822597</td>\n",
       "      <td>-1.427185</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>47</td>\n",
       "      <td>-2.484309</td>\n",
       "      <td>-1.586996</td>\n",
       "      <td>-1.998976</td>\n",
       "      <td>-1.372545</td>\n",
       "      <td>-6.984506</td>\n",
       "      <td>-1.322013</td>\n",
       "      <td>0.986663</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>2.244570</td>\n",
       "      <td>0.114871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>38.798088</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-4.213310</td>\n",
       "      <td>-0.309297</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>48</td>\n",
       "      <td>-3.023806</td>\n",
       "      <td>-0.325901</td>\n",
       "      <td>-3.148984</td>\n",
       "      <td>-0.270629</td>\n",
       "      <td>-6.467139</td>\n",
       "      <td>-0.331361</td>\n",
       "      <td>2.489530</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>1.594517</td>\n",
       "      <td>0.027433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22.992695</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>-4.309420</td>\n",
       "      <td>-1.574860</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>49</td>\n",
       "      <td>-2.340784</td>\n",
       "      <td>-1.774491</td>\n",
       "      <td>-2.043054</td>\n",
       "      <td>-1.496368</td>\n",
       "      <td>-8.544422</td>\n",
       "      <td>-1.453721</td>\n",
       "      <td>0.511447</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>2.997064</td>\n",
       "      <td>0.142230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>71.651513</td>\n",
       "      <td>0.099682</td>\n",
       "      <td>-4.459799</td>\n",
       "      <td>-0.364875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>50</td>\n",
       "      <td>-3.546979</td>\n",
       "      <td>-0.395417</td>\n",
       "      <td>-3.903283</td>\n",
       "      <td>-0.362912</td>\n",
       "      <td>-5.929134</td>\n",
       "      <td>-0.336296</td>\n",
       "      <td>2.258859</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>1.049110</td>\n",
       "      <td>0.024176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25.462525</td>\n",
       "      <td>0.067384</td>\n",
       "      <td>-4.502297</td>\n",
       "      <td>-1.510451</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>51</td>\n",
       "      <td>-2.159830</td>\n",
       "      <td>-1.690819</td>\n",
       "      <td>-1.962968</td>\n",
       "      <td>-1.440226</td>\n",
       "      <td>-9.384093</td>\n",
       "      <td>-1.400309</td>\n",
       "      <td>0.201606</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>3.452887</td>\n",
       "      <td>0.128576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>15.667155</td>\n",
       "      <td>0.068147</td>\n",
       "      <td>-4.560960</td>\n",
       "      <td>-1.584122</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>52</td>\n",
       "      <td>-4.294324</td>\n",
       "      <td>-1.768909</td>\n",
       "      <td>-2.001934</td>\n",
       "      <td>-1.523606</td>\n",
       "      <td>-7.386621</td>\n",
       "      <td>-1.459853</td>\n",
       "      <td>0.096616</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>2.206360</td>\n",
       "      <td>0.133231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>36.878244</td>\n",
       "      <td>0.076813</td>\n",
       "      <td>-4.562560</td>\n",
       "      <td>-1.228418</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>53</td>\n",
       "      <td>-2.741141</td>\n",
       "      <td>-1.376624</td>\n",
       "      <td>-2.266411</td>\n",
       "      <td>-1.157835</td>\n",
       "      <td>-8.680128</td>\n",
       "      <td>-1.150794</td>\n",
       "      <td>1.089255</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>2.918003</td>\n",
       "      <td>0.104837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>43.127504</td>\n",
       "      <td>0.073523</td>\n",
       "      <td>-4.660363</td>\n",
       "      <td>-1.125556</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>54</td>\n",
       "      <td>-2.512137</td>\n",
       "      <td>-1.266368</td>\n",
       "      <td>-2.389360</td>\n",
       "      <td>-1.055698</td>\n",
       "      <td>-9.079592</td>\n",
       "      <td>-1.054603</td>\n",
       "      <td>1.357779</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>3.125269</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>34.736308</td>\n",
       "      <td>0.042351</td>\n",
       "      <td>-4.854120</td>\n",
       "      <td>-0.776878</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>55</td>\n",
       "      <td>-3.435222</td>\n",
       "      <td>-0.872690</td>\n",
       "      <td>-2.595169</td>\n",
       "      <td>-0.748760</td>\n",
       "      <td>-8.531968</td>\n",
       "      <td>-0.709184</td>\n",
       "      <td>4.545600</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>2.623147</td>\n",
       "      <td>0.069649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>14.218470</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>-5.450553</td>\n",
       "      <td>-1.620568</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>56</td>\n",
       "      <td>-2.311396</td>\n",
       "      <td>-1.811218</td>\n",
       "      <td>-1.895459</td>\n",
       "      <td>-1.557541</td>\n",
       "      <td>-12.144804</td>\n",
       "      <td>-1.492946</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>4.736595</td>\n",
       "      <td>0.137365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>38.272984</td>\n",
       "      <td>0.089132</td>\n",
       "      <td>-5.478130</td>\n",
       "      <td>-0.890600</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>57</td>\n",
       "      <td>-3.338357</td>\n",
       "      <td>-0.995152</td>\n",
       "      <td>-3.055146</td>\n",
       "      <td>-0.855421</td>\n",
       "      <td>-10.040886</td>\n",
       "      <td>-0.821227</td>\n",
       "      <td>0.691896</td>\n",
       "      <td>0.006451</td>\n",
       "      <td>3.228427</td>\n",
       "      <td>0.075236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12.153365</td>\n",
       "      <td>0.061501</td>\n",
       "      <td>-5.622261</td>\n",
       "      <td>-1.716945</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>58</td>\n",
       "      <td>-2.832166</td>\n",
       "      <td>-1.962479</td>\n",
       "      <td>-2.433118</td>\n",
       "      <td>-1.635050</td>\n",
       "      <td>-11.601500</td>\n",
       "      <td>-1.553308</td>\n",
       "      <td>0.083211</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>4.231097</td>\n",
       "      <td>0.176796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>27.165137</td>\n",
       "      <td>0.075773</td>\n",
       "      <td>-6.196270</td>\n",
       "      <td>-1.276964</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>59</td>\n",
       "      <td>-2.803826</td>\n",
       "      <td>-1.439064</td>\n",
       "      <td>-4.120530</td>\n",
       "      <td>-1.217429</td>\n",
       "      <td>-11.664453</td>\n",
       "      <td>-1.174398</td>\n",
       "      <td>0.453586</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>3.903776</td>\n",
       "      <td>0.115960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>23.575107</td>\n",
       "      <td>0.072735</td>\n",
       "      <td>-6.425318</td>\n",
       "      <td>-1.406107</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>60</td>\n",
       "      <td>-3.287442</td>\n",
       "      <td>-1.605907</td>\n",
       "      <td>-5.093779</td>\n",
       "      <td>-1.331759</td>\n",
       "      <td>-10.894732</td>\n",
       "      <td>-1.280655</td>\n",
       "      <td>0.329811</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>3.245249</td>\n",
       "      <td>0.142812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "16      12.408733         0.042510        -1.727872         -1.621358   \n",
       "17      26.386224         0.068124        -1.731935         -1.482099   \n",
       "21      26.615493         0.068937        -1.791853         -1.558861   \n",
       "20      13.447075         0.051025        -1.803263         -1.674368   \n",
       "18      43.138781         0.077171        -1.814402         -1.307753   \n",
       "19      71.895877         0.086622        -1.841387         -1.099265   \n",
       "28      12.452810         0.058001        -1.842384         -1.583146   \n",
       "6       42.987909         0.078316        -1.859638         -1.696569   \n",
       "5       26.389244         0.060126        -1.861144         -1.748419   \n",
       "10      47.258625         0.094043        -1.897742         -1.749878   \n",
       "7       77.393106         0.096048        -1.905636         -1.584666   \n",
       "9       27.154617         0.062160        -1.929007         -1.814861   \n",
       "30      37.431016         0.071517        -1.929638         -1.253468   \n",
       "11      95.674379         0.104283        -1.930600         -1.664430   \n",
       "4       13.981733         0.047761        -1.943120         -1.822246   \n",
       "13      22.851825         0.055389        -1.954359         -1.621262   \n",
       "22      46.944064         0.089022        -1.961946         -1.408171   \n",
       "31      58.188299         0.081250        -1.970898         -1.042686   \n",
       "12      11.426562         0.054537        -1.979528         -1.768113   \n",
       "29      24.336335         0.072711        -1.983091         -1.420266   \n",
       "32      13.205518         0.052211        -1.991937         -1.639996   \n",
       "8       14.586561         0.061674        -2.032009         -1.962405   \n",
       "23      87.774307         0.105713        -2.074851         -1.148613   \n",
       "15      76.752856         0.087040        -2.190242         -1.232725   \n",
       "40      12.498520         0.057985        -2.269567         -1.541942   \n",
       "42      34.068051         0.084313        -2.354884         -1.202369   \n",
       "2       48.844235         0.088574        -2.460232         -2.324003   \n",
       "24      11.617735         0.058882        -2.474042         -1.728981   \n",
       "3       87.963250         0.116644        -2.482616         -2.241182   \n",
       "52      13.155516         0.056368        -2.493272         -1.505715   \n",
       "1       27.951357         0.064363        -2.496180         -2.407415   \n",
       "14      38.626225         0.064243        -2.511971         -1.476421   \n",
       "35      75.469110         0.095184        -2.528460         -0.970846   \n",
       "43      52.788463         0.074381        -2.530191         -1.002044   \n",
       "41      22.802124         0.066880        -2.553307         -1.379776   \n",
       "27      69.821680         0.085676        -2.744141         -1.057491   \n",
       "0       14.609785         0.081833        -2.763489         -2.647035   \n",
       "53      22.651472         0.077128        -2.850759         -1.333952   \n",
       "47      72.188706         0.079550        -3.122529         -0.720488   \n",
       "39      68.400009         0.094516        -3.141489         -0.795040   \n",
       "37      22.003787         0.061957        -3.181125         -1.526304   \n",
       "34      42.924056         0.070800        -3.306884         -1.314360   \n",
       "26      37.289499         0.072119        -3.337655         -1.371297   \n",
       "36      11.609967         0.051833        -3.381959         -1.710943   \n",
       "54      34.392599         0.091458        -3.533821         -1.119011   \n",
       "55      49.971275         0.078717        -3.750595         -0.913287   \n",
       "45      26.702921         0.063135        -3.822597         -1.427185   \n",
       "59      38.798088         0.033242        -4.213310         -0.309297   \n",
       "25      22.992695         0.053828        -4.309420         -1.574860   \n",
       "51      71.651513         0.099682        -4.459799         -0.364875   \n",
       "33      25.462525         0.067384        -4.502297         -1.510451   \n",
       "56      15.667155         0.068147        -4.560960         -1.584122   \n",
       "38      36.878244         0.076813        -4.562560         -1.228418   \n",
       "46      43.127504         0.073523        -4.660363         -1.125556   \n",
       "58      34.736308         0.042351        -4.854120         -0.776878   \n",
       "44      14.218470         0.063338        -5.450553         -1.620568   \n",
       "50      38.272984         0.089132        -5.478130         -0.890600   \n",
       "48      12.153365         0.061501        -5.622261         -1.716945   \n",
       "57      27.165137         0.075773        -6.196270         -1.276964   \n",
       "49      23.575107         0.072735        -6.425318         -1.406107   \n",
       "\n",
       "   param_learning_rate param_loss param_max_depth  \\\n",
       "16                0.05        lad               3   \n",
       "17                0.05        lad               5   \n",
       "21                0.05      huber               5   \n",
       "20                0.05      huber               3   \n",
       "18                0.05        lad               7   \n",
       "19                0.05        lad               9   \n",
       "28                 0.1        lad               3   \n",
       "6                 0.01        lad               7   \n",
       "5                 0.01        lad               5   \n",
       "10                0.01      huber               7   \n",
       "7                 0.01        lad               9   \n",
       "9                 0.01      huber               5   \n",
       "30                 0.1        lad               7   \n",
       "11                0.01      huber               9   \n",
       "4                 0.01        lad               3   \n",
       "13                0.05         ls               5   \n",
       "22                0.05      huber               7   \n",
       "31                 0.1        lad               9   \n",
       "12                0.05         ls               3   \n",
       "29                 0.1        lad               5   \n",
       "32                 0.1      huber               3   \n",
       "8                 0.01      huber               3   \n",
       "23                0.05      huber               9   \n",
       "15                0.05         ls               9   \n",
       "40                 0.2        lad               3   \n",
       "42                 0.2        lad               7   \n",
       "2                 0.01         ls               7   \n",
       "24                 0.1         ls               3   \n",
       "3                 0.01         ls               9   \n",
       "52                 0.5        lad               3   \n",
       "1                 0.01         ls               5   \n",
       "14                0.05         ls               7   \n",
       "35                 0.1      huber               9   \n",
       "43                 0.2        lad               9   \n",
       "41                 0.2        lad               5   \n",
       "27                 0.1         ls               9   \n",
       "0                 0.01         ls               3   \n",
       "53                 0.5        lad               5   \n",
       "47                 0.2      huber               9   \n",
       "39                 0.2         ls               9   \n",
       "37                 0.2         ls               5   \n",
       "34                 0.1      huber               7   \n",
       "26                 0.1         ls               7   \n",
       "36                 0.2         ls               3   \n",
       "54                 0.5        lad               7   \n",
       "55                 0.5        lad               9   \n",
       "45                 0.2      huber               5   \n",
       "59                 0.5      huber               9   \n",
       "25                 0.1         ls               5   \n",
       "51                 0.5         ls               9   \n",
       "33                 0.1      huber               5   \n",
       "56                 0.5      huber               3   \n",
       "38                 0.2         ls               7   \n",
       "46                 0.2      huber               7   \n",
       "58                 0.5      huber               7   \n",
       "44                 0.2      huber               3   \n",
       "50                 0.5         ls               7   \n",
       "48                 0.5         ls               3   \n",
       "57                 0.5      huber               5   \n",
       "49                 0.5         ls               5   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "16  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                1   \n",
       "17  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                2   \n",
       "21  {'learning_rate': 0.05, 'loss': 'huber', 'max_...                3   \n",
       "20  {'learning_rate': 0.05, 'loss': 'huber', 'max_...                4   \n",
       "18  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                5   \n",
       "19  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                6   \n",
       "28  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...                7   \n",
       "6   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...                8   \n",
       "5   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...                9   \n",
       "10  {'learning_rate': 0.01, 'loss': 'huber', 'max_...               10   \n",
       "7   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...               11   \n",
       "9   {'learning_rate': 0.01, 'loss': 'huber', 'max_...               12   \n",
       "30  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               13   \n",
       "11  {'learning_rate': 0.01, 'loss': 'huber', 'max_...               14   \n",
       "4   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...               15   \n",
       "13  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               16   \n",
       "22  {'learning_rate': 0.05, 'loss': 'huber', 'max_...               17   \n",
       "31  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               18   \n",
       "12  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               19   \n",
       "29  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               20   \n",
       "32  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               21   \n",
       "8   {'learning_rate': 0.01, 'loss': 'huber', 'max_...               22   \n",
       "23  {'learning_rate': 0.05, 'loss': 'huber', 'max_...               23   \n",
       "15  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               24   \n",
       "40  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               25   \n",
       "42  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               26   \n",
       "2   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               27   \n",
       "24  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               28   \n",
       "3   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               29   \n",
       "52  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               30   \n",
       "1   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               31   \n",
       "14  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               32   \n",
       "35  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               33   \n",
       "43  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               34   \n",
       "41  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               35   \n",
       "27  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               36   \n",
       "0   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               37   \n",
       "53  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               38   \n",
       "47  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               39   \n",
       "39  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               40   \n",
       "37  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               41   \n",
       "34  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               42   \n",
       "26  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               43   \n",
       "36  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               44   \n",
       "54  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               45   \n",
       "55  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               46   \n",
       "45  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               47   \n",
       "59  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               48   \n",
       "25  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               49   \n",
       "51  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               50   \n",
       "33  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               51   \n",
       "56  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               52   \n",
       "38  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               53   \n",
       "46  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               54   \n",
       "58  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               55   \n",
       "44  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               56   \n",
       "50  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               57   \n",
       "48  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               58   \n",
       "57  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               59   \n",
       "49  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               60   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "16          -1.470123           -1.842604          -1.768484   \n",
       "17          -1.513285           -1.651843          -1.756297   \n",
       "21          -1.524351           -1.743231          -1.853912   \n",
       "20          -1.590906           -1.871494          -1.803082   \n",
       "18          -1.730167           -1.446984          -1.748325   \n",
       "19          -1.801479           -1.209665          -1.779027   \n",
       "28          -1.462753           -1.759570          -1.799038   \n",
       "6           -1.789523           -1.886022          -1.826049   \n",
       "5           -1.841710           -1.941143          -1.811980   \n",
       "10          -1.814639           -1.954869          -1.891613   \n",
       "7           -1.825757           -1.758809          -1.872378   \n",
       "9           -1.901725           -2.015583          -1.909856   \n",
       "30          -1.932853           -1.389050          -1.781885   \n",
       "11          -1.831218           -1.868343          -1.899867   \n",
       "4           -2.037378           -2.049960          -1.817407   \n",
       "13          -1.617713           -1.819882          -1.878392   \n",
       "22          -1.948573           -1.572034          -1.808551   \n",
       "31          -2.042239           -1.159061          -1.833949   \n",
       "12          -1.661228           -1.990412          -1.941462   \n",
       "29          -2.012590           -1.580498          -1.763511   \n",
       "32          -1.775052           -1.839935          -1.812237   \n",
       "8           -2.178538           -2.203643          -1.909795   \n",
       "23          -2.147608           -1.276495          -1.886568   \n",
       "15          -2.063649           -1.370648          -1.958671   \n",
       "40          -1.579116           -1.714248          -1.789248   \n",
       "42          -2.878401           -1.312147          -1.856960   \n",
       "2           -2.653820           -2.689870          -2.331474   \n",
       "24          -1.647166           -1.938385          -2.000044   \n",
       "3           -2.643710           -2.590111          -2.346420   \n",
       "52          -2.864763           -1.694434          -1.930032   \n",
       "1           -2.780718           -2.770946          -2.362601   \n",
       "14          -1.859132           -1.656558          -1.948277   \n",
       "35          -2.656695           -1.076976          -2.069040   \n",
       "43          -2.131337           -1.115096          -1.869918   \n",
       "41          -2.004566           -1.536712          -1.843151   \n",
       "27          -2.304364           -1.166990          -2.191175   \n",
       "0           -3.235000           -3.015445          -2.533918   \n",
       "53          -2.217088           -1.526027          -1.922535   \n",
       "47          -2.630357           -0.777981          -2.218549   \n",
       "39          -3.006337           -0.882903          -2.361843   \n",
       "37          -2.595424           -1.728139          -2.305569   \n",
       "34          -2.436873           -1.454265          -1.895583   \n",
       "26          -2.548415           -1.525439          -2.017653   \n",
       "36          -2.202419           -1.933081          -1.983976   \n",
       "54          -2.984584           -1.252455          -2.236219   \n",
       "55          -2.130313           -1.015405          -2.233688   \n",
       "45          -2.484309           -1.586996          -1.998976   \n",
       "59          -3.023806           -0.325901          -3.148984   \n",
       "25          -2.340784           -1.774491          -2.043054   \n",
       "51          -3.546979           -0.395417          -3.903283   \n",
       "33          -2.159830           -1.690819          -1.962968   \n",
       "56          -4.294324           -1.768909          -2.001934   \n",
       "38          -2.741141           -1.376624          -2.266411   \n",
       "46          -2.512137           -1.266368          -2.389360   \n",
       "58          -3.435222           -0.872690          -2.595169   \n",
       "44          -2.311396           -1.811218          -1.895459   \n",
       "50          -3.338357           -0.995152          -3.055146   \n",
       "48          -2.832166           -1.962479          -2.433118   \n",
       "57          -2.803826           -1.439064          -4.120530   \n",
       "49          -3.287442           -1.605907          -5.093779   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "16           -1.551389          -1.945008           -1.470083      0.562040   \n",
       "17           -1.430296          -1.926222           -1.364159      0.340439   \n",
       "21           -1.502879          -1.997297           -1.430474      0.381171   \n",
       "20           -1.607305          -2.015801           -1.544305      0.313712   \n",
       "18           -1.257051          -1.964714           -1.219225      0.564184   \n",
       "19           -1.044392          -1.943655           -1.043736      0.833252   \n",
       "28           -1.531443          -2.265361           -1.458426      0.168182   \n",
       "6            -1.626721          -1.963342           -1.576964      1.424292   \n",
       "5            -1.675511          -1.929742           -1.628602      0.691079   \n",
       "10           -1.671992          -1.986974           -1.622775      0.847066   \n",
       "7            -1.508971          -2.018774           -1.486219      1.495793   \n",
       "9            -1.742419          -1.975440           -1.686583      0.505933   \n",
       "30           -1.195953          -2.074177           -1.175401      0.363565   \n",
       "11           -1.582423          -2.060716           -1.542524      1.804403   \n",
       "4            -1.703278          -1.974575           -1.713500      0.151283   \n",
       "13           -1.554622          -2.366973           -1.489281      0.601397   \n",
       "22           -1.349839          -2.128712           -1.302641      0.582564   \n",
       "31           -0.992898          -2.036506           -0.976100      0.412954   \n",
       "12           -1.699707          -2.335895           -1.614222      0.407961   \n",
       "29           -1.369703          -2.173171           -1.310598      0.468893   \n",
       "32           -1.563534          -2.388522           -1.516520      0.182999   \n",
       "8            -1.864175          -2.007694           -1.819397      0.050646   \n",
       "23           -1.082055          -2.190377           -1.087287      1.572514   \n",
       "15           -1.167216          -2.548407           -1.160312      0.985835   \n",
       "40           -1.489417          -3.440339           -1.422161      0.213543   \n",
       "42           -1.162457          -2.329291           -1.132502      0.816304   \n",
       "2            -2.199878          -2.395401           -2.082263      1.860523   \n",
       "24           -1.657973          -3.774918           -1.590583      0.160408   \n",
       "3            -2.115677          -2.457720           -2.017758      1.874916   \n",
       "52           -1.430251          -2.685020           -1.392460      0.068936   \n",
       "1            -2.293836          -2.345221           -2.157461      0.334876   \n",
       "14           -1.409389          -3.728505           -1.363316      1.064254   \n",
       "35           -0.907608          -2.859644           -0.927955      1.775050   \n",
       "43           -0.967361          -3.589319           -0.923676      1.082908   \n",
       "41           -1.319659          -3.812204           -1.282957      0.315904   \n",
       "27           -1.009453          -3.736884           -0.996029      0.458487   \n",
       "0            -2.528554          -2.521550           -2.397105      0.221060   \n",
       "53           -1.246456          -4.412653           -1.229374      0.184523   \n",
       "47           -0.685091          -4.518682           -0.698393      1.868461   \n",
       "39           -0.744874          -4.056287           -0.757343      0.508664   \n",
       "37           -1.457869          -4.642382           -1.392903      0.145569   \n",
       "34           -1.251570          -5.588196           -1.237246      1.595740   \n",
       "26           -1.307659          -5.446897           -1.280793      1.150061   \n",
       "36           -1.626177          -5.959483           -1.573571      0.201665   \n",
       "54           -1.080975          -5.380660           -1.023601      0.587914   \n",
       "55           -0.910113          -6.887783           -0.814342      0.785924   \n",
       "45           -1.372545          -6.984506           -1.322013      0.986663   \n",
       "59           -0.270629          -6.467139           -0.331361      2.489530   \n",
       "25           -1.496368          -8.544422           -1.453721      0.511447   \n",
       "51           -0.362912          -5.929134           -0.336296      2.258859   \n",
       "33           -1.440226          -9.384093           -1.400309      0.201606   \n",
       "56           -1.523606          -7.386621           -1.459853      0.096616   \n",
       "38           -1.157835          -8.680128           -1.150794      1.089255   \n",
       "46           -1.055698          -9.079592           -1.054603      1.357779   \n",
       "58           -0.748760          -8.531968           -0.709184      4.545600   \n",
       "44           -1.557541         -12.144804           -1.492946      0.459823   \n",
       "50           -0.855421         -10.040886           -0.821227      0.691896   \n",
       "48           -1.635050         -11.601500           -1.553308      0.083211   \n",
       "57           -1.217429         -11.664453           -1.174398      0.453586   \n",
       "49           -1.331759         -10.894732           -1.280655      0.329811   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "16        0.003591        0.195986         0.159926  \n",
       "17        0.007063        0.169459         0.123026  \n",
       "21        0.003982        0.198003         0.133678  \n",
       "20        0.003979        0.173462         0.141742  \n",
       "18        0.003952        0.106545         0.099655  \n",
       "19        0.002965        0.072893         0.078066  \n",
       "28        0.002616        0.329093         0.128262  \n",
       "6         0.006486        0.074831         0.135495  \n",
       "5         0.002123        0.050001         0.137616  \n",
       "10        0.011767        0.070489         0.146336  \n",
       "7         0.012198        0.082233         0.123488  \n",
       "9         0.005135        0.033001         0.143750  \n",
       "30        0.005707        0.119349         0.096237  \n",
       "11        0.011514        0.096179         0.145106  \n",
       "4         0.004501        0.092516         0.161072  \n",
       "13        0.007060        0.310565         0.142956  \n",
       "22        0.010738        0.131047         0.117460  \n",
       "31        0.003512        0.096866         0.082575  \n",
       "12        0.001715        0.276744         0.161016  \n",
       "29        0.003010        0.168539         0.115842  \n",
       "32        0.005012        0.280838         0.142675  \n",
       "8         0.010459        0.111053         0.171558  \n",
       "23        0.009311        0.134277         0.090452  \n",
       "15        0.011798        0.256861         0.097567  \n",
       "40        0.006084        0.832293         0.124894  \n",
       "42        0.005109        0.417394         0.078582  \n",
       "2         0.006081        0.139354         0.263125  \n",
       "24        0.013704        0.931070         0.150606  \n",
       "3         0.019472        0.122638         0.249947  \n",
       "52        0.004754        0.404974         0.134334  \n",
       "1         0.001329        0.201324         0.263016  \n",
       "14        0.007902        0.860989         0.128757  \n",
       "35        0.007215        0.335258         0.075504  \n",
       "43        0.006827        0.756482         0.081905  \n",
       "41        0.006820        0.892610         0.111977  \n",
       "27        0.006745        0.703495         0.077622  \n",
       "0         0.017211        0.333447         0.265975  \n",
       "53        0.011255        1.110953         0.135996  \n",
       "47        0.004669        1.001442         0.041014  \n",
       "39        0.014728        0.698324         0.062337  \n",
       "37        0.010590        1.040018         0.145163  \n",
       "34        0.004000        1.628197         0.099100  \n",
       "26        0.002122        1.507117         0.109545  \n",
       "36        0.003727        1.824765         0.158537  \n",
       "54        0.007148        1.341174         0.097223  \n",
       "55        0.009275        2.218728         0.082114  \n",
       "45        0.000674        2.244570         0.114871  \n",
       "59        0.001674        1.594517         0.027433  \n",
       "25        0.002862        2.997064         0.142230  \n",
       "51        0.007486        1.049110         0.024176  \n",
       "33        0.004532        3.452887         0.128576  \n",
       "56        0.002109        2.206360         0.133231  \n",
       "38        0.006218        2.918003         0.104837  \n",
       "46        0.000151        3.125269         0.099570  \n",
       "58        0.001748        2.623147         0.069649  \n",
       "44        0.002777        4.736595         0.137365  \n",
       "50        0.006451        3.228427         0.075236  \n",
       "48        0.008621        4.231097         0.176796  \n",
       "57        0.007811        3.903776         0.115960  \n",
       "49        0.005448        3.245249         0.142812  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['ls', 'lad', 'huber'], \n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "}\n",
    "rf = GradientBoostingRegressor(n_estimators=100)\n",
    "cv_search = GridSearchCV(rf, param_grid,  n_jobs=-1,\n",
    "                         scoring='neg_median_absolute_error', cv=3, \n",
    "                         verbose=2, error_score = -999 )\n",
    "cv_search = cv_search.fit(X=features, y=target)\n",
    "search_results = pd.DataFrame(cv_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.408733</td>\n",
       "      <td>0.042510</td>\n",
       "      <td>-1.727872</td>\n",
       "      <td>-1.621358</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.470123</td>\n",
       "      <td>-1.842604</td>\n",
       "      <td>-1.768484</td>\n",
       "      <td>-1.551389</td>\n",
       "      <td>-1.945008</td>\n",
       "      <td>-1.470083</td>\n",
       "      <td>0.562040</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.195986</td>\n",
       "      <td>0.159926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26.386224</td>\n",
       "      <td>0.068124</td>\n",
       "      <td>-1.731935</td>\n",
       "      <td>-1.482099</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.513285</td>\n",
       "      <td>-1.651843</td>\n",
       "      <td>-1.756297</td>\n",
       "      <td>-1.430296</td>\n",
       "      <td>-1.926222</td>\n",
       "      <td>-1.364159</td>\n",
       "      <td>0.340439</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.169459</td>\n",
       "      <td>0.123026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26.615493</td>\n",
       "      <td>0.068937</td>\n",
       "      <td>-1.791853</td>\n",
       "      <td>-1.558861</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.524351</td>\n",
       "      <td>-1.743231</td>\n",
       "      <td>-1.853912</td>\n",
       "      <td>-1.502879</td>\n",
       "      <td>-1.997297</td>\n",
       "      <td>-1.430474</td>\n",
       "      <td>0.381171</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.198003</td>\n",
       "      <td>0.133678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.447075</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>-1.803263</td>\n",
       "      <td>-1.674368</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.590906</td>\n",
       "      <td>-1.871494</td>\n",
       "      <td>-1.803082</td>\n",
       "      <td>-1.607305</td>\n",
       "      <td>-2.015801</td>\n",
       "      <td>-1.544305</td>\n",
       "      <td>0.313712</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.173462</td>\n",
       "      <td>0.141742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.138781</td>\n",
       "      <td>0.077171</td>\n",
       "      <td>-1.814402</td>\n",
       "      <td>-1.307753</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.730167</td>\n",
       "      <td>-1.446984</td>\n",
       "      <td>-1.748325</td>\n",
       "      <td>-1.257051</td>\n",
       "      <td>-1.964714</td>\n",
       "      <td>-1.219225</td>\n",
       "      <td>0.564184</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.106545</td>\n",
       "      <td>0.099655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>71.895877</td>\n",
       "      <td>0.086622</td>\n",
       "      <td>-1.841387</td>\n",
       "      <td>-1.099265</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.801479</td>\n",
       "      <td>-1.209665</td>\n",
       "      <td>-1.779027</td>\n",
       "      <td>-1.044392</td>\n",
       "      <td>-1.943655</td>\n",
       "      <td>-1.043736</td>\n",
       "      <td>0.833252</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.072893</td>\n",
       "      <td>0.078066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.452810</td>\n",
       "      <td>0.058001</td>\n",
       "      <td>-1.842384</td>\n",
       "      <td>-1.583146</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.462753</td>\n",
       "      <td>-1.759570</td>\n",
       "      <td>-1.799038</td>\n",
       "      <td>-1.531443</td>\n",
       "      <td>-2.265361</td>\n",
       "      <td>-1.458426</td>\n",
       "      <td>0.168182</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.329093</td>\n",
       "      <td>0.128262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.987909</td>\n",
       "      <td>0.078316</td>\n",
       "      <td>-1.859638</td>\n",
       "      <td>-1.696569</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.789523</td>\n",
       "      <td>-1.886022</td>\n",
       "      <td>-1.826049</td>\n",
       "      <td>-1.626721</td>\n",
       "      <td>-1.963342</td>\n",
       "      <td>-1.576964</td>\n",
       "      <td>1.424292</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>0.074831</td>\n",
       "      <td>0.135495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.389244</td>\n",
       "      <td>0.060126</td>\n",
       "      <td>-1.861144</td>\n",
       "      <td>-1.748419</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.841710</td>\n",
       "      <td>-1.941143</td>\n",
       "      <td>-1.811980</td>\n",
       "      <td>-1.675511</td>\n",
       "      <td>-1.929742</td>\n",
       "      <td>-1.628602</td>\n",
       "      <td>0.691079</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.137616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.258625</td>\n",
       "      <td>0.094043</td>\n",
       "      <td>-1.897742</td>\n",
       "      <td>-1.749878</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.814639</td>\n",
       "      <td>-1.954869</td>\n",
       "      <td>-1.891613</td>\n",
       "      <td>-1.671992</td>\n",
       "      <td>-1.986974</td>\n",
       "      <td>-1.622775</td>\n",
       "      <td>0.847066</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.146336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77.393106</td>\n",
       "      <td>0.096048</td>\n",
       "      <td>-1.905636</td>\n",
       "      <td>-1.584666</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.825757</td>\n",
       "      <td>-1.758809</td>\n",
       "      <td>-1.872378</td>\n",
       "      <td>-1.508971</td>\n",
       "      <td>-2.018774</td>\n",
       "      <td>-1.486219</td>\n",
       "      <td>1.495793</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.082233</td>\n",
       "      <td>0.123488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.154617</td>\n",
       "      <td>0.062160</td>\n",
       "      <td>-1.929007</td>\n",
       "      <td>-1.814861</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.901725</td>\n",
       "      <td>-2.015583</td>\n",
       "      <td>-1.909856</td>\n",
       "      <td>-1.742419</td>\n",
       "      <td>-1.975440</td>\n",
       "      <td>-1.686583</td>\n",
       "      <td>0.505933</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>37.431016</td>\n",
       "      <td>0.071517</td>\n",
       "      <td>-1.929638</td>\n",
       "      <td>-1.253468</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.932853</td>\n",
       "      <td>-1.389050</td>\n",
       "      <td>-1.781885</td>\n",
       "      <td>-1.195953</td>\n",
       "      <td>-2.074177</td>\n",
       "      <td>-1.175401</td>\n",
       "      <td>0.363565</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>0.119349</td>\n",
       "      <td>0.096237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>95.674379</td>\n",
       "      <td>0.104283</td>\n",
       "      <td>-1.930600</td>\n",
       "      <td>-1.664430</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.831218</td>\n",
       "      <td>-1.868343</td>\n",
       "      <td>-1.899867</td>\n",
       "      <td>-1.582423</td>\n",
       "      <td>-2.060716</td>\n",
       "      <td>-1.542524</td>\n",
       "      <td>1.804403</td>\n",
       "      <td>0.011514</td>\n",
       "      <td>0.096179</td>\n",
       "      <td>0.145106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.981733</td>\n",
       "      <td>0.047761</td>\n",
       "      <td>-1.943120</td>\n",
       "      <td>-1.822246</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>15</td>\n",
       "      <td>-2.037378</td>\n",
       "      <td>-2.049960</td>\n",
       "      <td>-1.817407</td>\n",
       "      <td>-1.703278</td>\n",
       "      <td>-1.974575</td>\n",
       "      <td>-1.713500</td>\n",
       "      <td>0.151283</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>0.092516</td>\n",
       "      <td>0.161072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22.851825</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>-1.954359</td>\n",
       "      <td>-1.621262</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.617713</td>\n",
       "      <td>-1.819882</td>\n",
       "      <td>-1.878392</td>\n",
       "      <td>-1.554622</td>\n",
       "      <td>-2.366973</td>\n",
       "      <td>-1.489281</td>\n",
       "      <td>0.601397</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.310565</td>\n",
       "      <td>0.142956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>46.944064</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>-1.961946</td>\n",
       "      <td>-1.408171</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.948573</td>\n",
       "      <td>-1.572034</td>\n",
       "      <td>-1.808551</td>\n",
       "      <td>-1.349839</td>\n",
       "      <td>-2.128712</td>\n",
       "      <td>-1.302641</td>\n",
       "      <td>0.582564</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.131047</td>\n",
       "      <td>0.117460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>58.188299</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>-1.970898</td>\n",
       "      <td>-1.042686</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>18</td>\n",
       "      <td>-2.042239</td>\n",
       "      <td>-1.159061</td>\n",
       "      <td>-1.833949</td>\n",
       "      <td>-0.992898</td>\n",
       "      <td>-2.036506</td>\n",
       "      <td>-0.976100</td>\n",
       "      <td>0.412954</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.096866</td>\n",
       "      <td>0.082575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.426562</td>\n",
       "      <td>0.054537</td>\n",
       "      <td>-1.979528</td>\n",
       "      <td>-1.768113</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>19</td>\n",
       "      <td>-1.661228</td>\n",
       "      <td>-1.990412</td>\n",
       "      <td>-1.941462</td>\n",
       "      <td>-1.699707</td>\n",
       "      <td>-2.335895</td>\n",
       "      <td>-1.614222</td>\n",
       "      <td>0.407961</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.276744</td>\n",
       "      <td>0.161016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.336335</td>\n",
       "      <td>0.072711</td>\n",
       "      <td>-1.983091</td>\n",
       "      <td>-1.420266</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>20</td>\n",
       "      <td>-2.012590</td>\n",
       "      <td>-1.580498</td>\n",
       "      <td>-1.763511</td>\n",
       "      <td>-1.369703</td>\n",
       "      <td>-2.173171</td>\n",
       "      <td>-1.310598</td>\n",
       "      <td>0.468893</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.115842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13.205518</td>\n",
       "      <td>0.052211</td>\n",
       "      <td>-1.991937</td>\n",
       "      <td>-1.639996</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.775052</td>\n",
       "      <td>-1.839935</td>\n",
       "      <td>-1.812237</td>\n",
       "      <td>-1.563534</td>\n",
       "      <td>-2.388522</td>\n",
       "      <td>-1.516520</td>\n",
       "      <td>0.182999</td>\n",
       "      <td>0.005012</td>\n",
       "      <td>0.280838</td>\n",
       "      <td>0.142675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.586561</td>\n",
       "      <td>0.061674</td>\n",
       "      <td>-2.032009</td>\n",
       "      <td>-1.962405</td>\n",
       "      <td>0.01</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'huber', 'max_...</td>\n",
       "      <td>22</td>\n",
       "      <td>-2.178538</td>\n",
       "      <td>-2.203643</td>\n",
       "      <td>-1.909795</td>\n",
       "      <td>-1.864175</td>\n",
       "      <td>-2.007694</td>\n",
       "      <td>-1.819397</td>\n",
       "      <td>0.050646</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.111053</td>\n",
       "      <td>0.171558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>87.774307</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>-2.074851</td>\n",
       "      <td>-1.148613</td>\n",
       "      <td>0.05</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'huber', 'max_...</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.147608</td>\n",
       "      <td>-1.276495</td>\n",
       "      <td>-1.886568</td>\n",
       "      <td>-1.082055</td>\n",
       "      <td>-2.190377</td>\n",
       "      <td>-1.087287</td>\n",
       "      <td>1.572514</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.090452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>76.752856</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>-2.190242</td>\n",
       "      <td>-1.232725</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>24</td>\n",
       "      <td>-2.063649</td>\n",
       "      <td>-1.370648</td>\n",
       "      <td>-1.958671</td>\n",
       "      <td>-1.167216</td>\n",
       "      <td>-2.548407</td>\n",
       "      <td>-1.160312</td>\n",
       "      <td>0.985835</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>0.256861</td>\n",
       "      <td>0.097567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12.498520</td>\n",
       "      <td>0.057985</td>\n",
       "      <td>-2.269567</td>\n",
       "      <td>-1.541942</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>25</td>\n",
       "      <td>-1.579116</td>\n",
       "      <td>-1.714248</td>\n",
       "      <td>-1.789248</td>\n",
       "      <td>-1.489417</td>\n",
       "      <td>-3.440339</td>\n",
       "      <td>-1.422161</td>\n",
       "      <td>0.213543</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.832293</td>\n",
       "      <td>0.124894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>34.068051</td>\n",
       "      <td>0.084313</td>\n",
       "      <td>-2.354884</td>\n",
       "      <td>-1.202369</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>26</td>\n",
       "      <td>-2.878401</td>\n",
       "      <td>-1.312147</td>\n",
       "      <td>-1.856960</td>\n",
       "      <td>-1.162457</td>\n",
       "      <td>-2.329291</td>\n",
       "      <td>-1.132502</td>\n",
       "      <td>0.816304</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.417394</td>\n",
       "      <td>0.078582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.844235</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>-2.460232</td>\n",
       "      <td>-2.324003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>27</td>\n",
       "      <td>-2.653820</td>\n",
       "      <td>-2.689870</td>\n",
       "      <td>-2.331474</td>\n",
       "      <td>-2.199878</td>\n",
       "      <td>-2.395401</td>\n",
       "      <td>-2.082263</td>\n",
       "      <td>1.860523</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.139354</td>\n",
       "      <td>0.263125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.617735</td>\n",
       "      <td>0.058882</td>\n",
       "      <td>-2.474042</td>\n",
       "      <td>-1.728981</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>28</td>\n",
       "      <td>-1.647166</td>\n",
       "      <td>-1.938385</td>\n",
       "      <td>-2.000044</td>\n",
       "      <td>-1.657973</td>\n",
       "      <td>-3.774918</td>\n",
       "      <td>-1.590583</td>\n",
       "      <td>0.160408</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.931070</td>\n",
       "      <td>0.150606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.963250</td>\n",
       "      <td>0.116644</td>\n",
       "      <td>-2.482616</td>\n",
       "      <td>-2.241182</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>29</td>\n",
       "      <td>-2.643710</td>\n",
       "      <td>-2.590111</td>\n",
       "      <td>-2.346420</td>\n",
       "      <td>-2.115677</td>\n",
       "      <td>-2.457720</td>\n",
       "      <td>-2.017758</td>\n",
       "      <td>1.874916</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.122638</td>\n",
       "      <td>0.249947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.155516</td>\n",
       "      <td>0.056368</td>\n",
       "      <td>-2.493272</td>\n",
       "      <td>-1.505715</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>30</td>\n",
       "      <td>-2.864763</td>\n",
       "      <td>-1.694434</td>\n",
       "      <td>-1.930032</td>\n",
       "      <td>-1.430251</td>\n",
       "      <td>-2.685020</td>\n",
       "      <td>-1.392460</td>\n",
       "      <td>0.068936</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>0.404974</td>\n",
       "      <td>0.134334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.951357</td>\n",
       "      <td>0.064363</td>\n",
       "      <td>-2.496180</td>\n",
       "      <td>-2.407415</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>31</td>\n",
       "      <td>-2.780718</td>\n",
       "      <td>-2.770946</td>\n",
       "      <td>-2.362601</td>\n",
       "      <td>-2.293836</td>\n",
       "      <td>-2.345221</td>\n",
       "      <td>-2.157461</td>\n",
       "      <td>0.334876</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.201324</td>\n",
       "      <td>0.263016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.626225</td>\n",
       "      <td>0.064243</td>\n",
       "      <td>-2.511971</td>\n",
       "      <td>-1.476421</td>\n",
       "      <td>0.05</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>32</td>\n",
       "      <td>-1.859132</td>\n",
       "      <td>-1.656558</td>\n",
       "      <td>-1.948277</td>\n",
       "      <td>-1.409389</td>\n",
       "      <td>-3.728505</td>\n",
       "      <td>-1.363316</td>\n",
       "      <td>1.064254</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.860989</td>\n",
       "      <td>0.128757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>75.469110</td>\n",
       "      <td>0.095184</td>\n",
       "      <td>-2.528460</td>\n",
       "      <td>-0.970846</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>33</td>\n",
       "      <td>-2.656695</td>\n",
       "      <td>-1.076976</td>\n",
       "      <td>-2.069040</td>\n",
       "      <td>-0.907608</td>\n",
       "      <td>-2.859644</td>\n",
       "      <td>-0.927955</td>\n",
       "      <td>1.775050</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.335258</td>\n",
       "      <td>0.075504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>52.788463</td>\n",
       "      <td>0.074381</td>\n",
       "      <td>-2.530191</td>\n",
       "      <td>-1.002044</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>34</td>\n",
       "      <td>-2.131337</td>\n",
       "      <td>-1.115096</td>\n",
       "      <td>-1.869918</td>\n",
       "      <td>-0.967361</td>\n",
       "      <td>-3.589319</td>\n",
       "      <td>-0.923676</td>\n",
       "      <td>1.082908</td>\n",
       "      <td>0.006827</td>\n",
       "      <td>0.756482</td>\n",
       "      <td>0.081905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>22.802124</td>\n",
       "      <td>0.066880</td>\n",
       "      <td>-2.553307</td>\n",
       "      <td>-1.379776</td>\n",
       "      <td>0.2</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>35</td>\n",
       "      <td>-2.004566</td>\n",
       "      <td>-1.536712</td>\n",
       "      <td>-1.843151</td>\n",
       "      <td>-1.319659</td>\n",
       "      <td>-3.812204</td>\n",
       "      <td>-1.282957</td>\n",
       "      <td>0.315904</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.892610</td>\n",
       "      <td>0.111977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>69.821680</td>\n",
       "      <td>0.085676</td>\n",
       "      <td>-2.744141</td>\n",
       "      <td>-1.057491</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>36</td>\n",
       "      <td>-2.304364</td>\n",
       "      <td>-1.166990</td>\n",
       "      <td>-2.191175</td>\n",
       "      <td>-1.009453</td>\n",
       "      <td>-3.736884</td>\n",
       "      <td>-0.996029</td>\n",
       "      <td>0.458487</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.703495</td>\n",
       "      <td>0.077622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.609785</td>\n",
       "      <td>0.081833</td>\n",
       "      <td>-2.763489</td>\n",
       "      <td>-2.647035</td>\n",
       "      <td>0.01</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.01, 'loss': 'ls', 'max_dep...</td>\n",
       "      <td>37</td>\n",
       "      <td>-3.235000</td>\n",
       "      <td>-3.015445</td>\n",
       "      <td>-2.533918</td>\n",
       "      <td>-2.528554</td>\n",
       "      <td>-2.521550</td>\n",
       "      <td>-2.397105</td>\n",
       "      <td>0.221060</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.333447</td>\n",
       "      <td>0.265975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.651472</td>\n",
       "      <td>0.077128</td>\n",
       "      <td>-2.850759</td>\n",
       "      <td>-1.333952</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>38</td>\n",
       "      <td>-2.217088</td>\n",
       "      <td>-1.526027</td>\n",
       "      <td>-1.922535</td>\n",
       "      <td>-1.246456</td>\n",
       "      <td>-4.412653</td>\n",
       "      <td>-1.229374</td>\n",
       "      <td>0.184523</td>\n",
       "      <td>0.011255</td>\n",
       "      <td>1.110953</td>\n",
       "      <td>0.135996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>72.188706</td>\n",
       "      <td>0.079550</td>\n",
       "      <td>-3.122529</td>\n",
       "      <td>-0.720488</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>39</td>\n",
       "      <td>-2.630357</td>\n",
       "      <td>-0.777981</td>\n",
       "      <td>-2.218549</td>\n",
       "      <td>-0.685091</td>\n",
       "      <td>-4.518682</td>\n",
       "      <td>-0.698393</td>\n",
       "      <td>1.868461</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>1.001442</td>\n",
       "      <td>0.041014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>68.400009</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>-3.141489</td>\n",
       "      <td>-0.795040</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>40</td>\n",
       "      <td>-3.006337</td>\n",
       "      <td>-0.882903</td>\n",
       "      <td>-2.361843</td>\n",
       "      <td>-0.744874</td>\n",
       "      <td>-4.056287</td>\n",
       "      <td>-0.757343</td>\n",
       "      <td>0.508664</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>0.062337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>22.003787</td>\n",
       "      <td>0.061957</td>\n",
       "      <td>-3.181125</td>\n",
       "      <td>-1.526304</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>41</td>\n",
       "      <td>-2.595424</td>\n",
       "      <td>-1.728139</td>\n",
       "      <td>-2.305569</td>\n",
       "      <td>-1.457869</td>\n",
       "      <td>-4.642382</td>\n",
       "      <td>-1.392903</td>\n",
       "      <td>0.145569</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>1.040018</td>\n",
       "      <td>0.145163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>42.924056</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>-3.306884</td>\n",
       "      <td>-1.314360</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>42</td>\n",
       "      <td>-2.436873</td>\n",
       "      <td>-1.454265</td>\n",
       "      <td>-1.895583</td>\n",
       "      <td>-1.251570</td>\n",
       "      <td>-5.588196</td>\n",
       "      <td>-1.237246</td>\n",
       "      <td>1.595740</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.628197</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37.289499</td>\n",
       "      <td>0.072119</td>\n",
       "      <td>-3.337655</td>\n",
       "      <td>-1.371297</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>43</td>\n",
       "      <td>-2.548415</td>\n",
       "      <td>-1.525439</td>\n",
       "      <td>-2.017653</td>\n",
       "      <td>-1.307659</td>\n",
       "      <td>-5.446897</td>\n",
       "      <td>-1.280793</td>\n",
       "      <td>1.150061</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>1.507117</td>\n",
       "      <td>0.109545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11.609967</td>\n",
       "      <td>0.051833</td>\n",
       "      <td>-3.381959</td>\n",
       "      <td>-1.710943</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>44</td>\n",
       "      <td>-2.202419</td>\n",
       "      <td>-1.933081</td>\n",
       "      <td>-1.983976</td>\n",
       "      <td>-1.626177</td>\n",
       "      <td>-5.959483</td>\n",
       "      <td>-1.573571</td>\n",
       "      <td>0.201665</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>1.824765</td>\n",
       "      <td>0.158537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>34.392599</td>\n",
       "      <td>0.091458</td>\n",
       "      <td>-3.533821</td>\n",
       "      <td>-1.119011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>45</td>\n",
       "      <td>-2.984584</td>\n",
       "      <td>-1.252455</td>\n",
       "      <td>-2.236219</td>\n",
       "      <td>-1.080975</td>\n",
       "      <td>-5.380660</td>\n",
       "      <td>-1.023601</td>\n",
       "      <td>0.587914</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>1.341174</td>\n",
       "      <td>0.097223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>49.971275</td>\n",
       "      <td>0.078717</td>\n",
       "      <td>-3.750595</td>\n",
       "      <td>-0.913287</td>\n",
       "      <td>0.5</td>\n",
       "      <td>lad</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'lad', 'max_dep...</td>\n",
       "      <td>46</td>\n",
       "      <td>-2.130313</td>\n",
       "      <td>-1.015405</td>\n",
       "      <td>-2.233688</td>\n",
       "      <td>-0.910113</td>\n",
       "      <td>-6.887783</td>\n",
       "      <td>-0.814342</td>\n",
       "      <td>0.785924</td>\n",
       "      <td>0.009275</td>\n",
       "      <td>2.218728</td>\n",
       "      <td>0.082114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.702921</td>\n",
       "      <td>0.063135</td>\n",
       "      <td>-3.822597</td>\n",
       "      <td>-1.427185</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>47</td>\n",
       "      <td>-2.484309</td>\n",
       "      <td>-1.586996</td>\n",
       "      <td>-1.998976</td>\n",
       "      <td>-1.372545</td>\n",
       "      <td>-6.984506</td>\n",
       "      <td>-1.322013</td>\n",
       "      <td>0.986663</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>2.244570</td>\n",
       "      <td>0.114871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>38.798088</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-4.213310</td>\n",
       "      <td>-0.309297</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>48</td>\n",
       "      <td>-3.023806</td>\n",
       "      <td>-0.325901</td>\n",
       "      <td>-3.148984</td>\n",
       "      <td>-0.270629</td>\n",
       "      <td>-6.467139</td>\n",
       "      <td>-0.331361</td>\n",
       "      <td>2.489530</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>1.594517</td>\n",
       "      <td>0.027433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22.992695</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>-4.309420</td>\n",
       "      <td>-1.574860</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>49</td>\n",
       "      <td>-2.340784</td>\n",
       "      <td>-1.774491</td>\n",
       "      <td>-2.043054</td>\n",
       "      <td>-1.496368</td>\n",
       "      <td>-8.544422</td>\n",
       "      <td>-1.453721</td>\n",
       "      <td>0.511447</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>2.997064</td>\n",
       "      <td>0.142230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>71.651513</td>\n",
       "      <td>0.099682</td>\n",
       "      <td>-4.459799</td>\n",
       "      <td>-0.364875</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>9</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>50</td>\n",
       "      <td>-3.546979</td>\n",
       "      <td>-0.395417</td>\n",
       "      <td>-3.903283</td>\n",
       "      <td>-0.362912</td>\n",
       "      <td>-5.929134</td>\n",
       "      <td>-0.336296</td>\n",
       "      <td>2.258859</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>1.049110</td>\n",
       "      <td>0.024176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25.462525</td>\n",
       "      <td>0.067384</td>\n",
       "      <td>-4.502297</td>\n",
       "      <td>-1.510451</td>\n",
       "      <td>0.1</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.1, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>51</td>\n",
       "      <td>-2.159830</td>\n",
       "      <td>-1.690819</td>\n",
       "      <td>-1.962968</td>\n",
       "      <td>-1.440226</td>\n",
       "      <td>-9.384093</td>\n",
       "      <td>-1.400309</td>\n",
       "      <td>0.201606</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>3.452887</td>\n",
       "      <td>0.128576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>15.667155</td>\n",
       "      <td>0.068147</td>\n",
       "      <td>-4.560960</td>\n",
       "      <td>-1.584122</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>52</td>\n",
       "      <td>-4.294324</td>\n",
       "      <td>-1.768909</td>\n",
       "      <td>-2.001934</td>\n",
       "      <td>-1.523606</td>\n",
       "      <td>-7.386621</td>\n",
       "      <td>-1.459853</td>\n",
       "      <td>0.096616</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>2.206360</td>\n",
       "      <td>0.133231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>36.878244</td>\n",
       "      <td>0.076813</td>\n",
       "      <td>-4.562560</td>\n",
       "      <td>-1.228418</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>53</td>\n",
       "      <td>-2.741141</td>\n",
       "      <td>-1.376624</td>\n",
       "      <td>-2.266411</td>\n",
       "      <td>-1.157835</td>\n",
       "      <td>-8.680128</td>\n",
       "      <td>-1.150794</td>\n",
       "      <td>1.089255</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>2.918003</td>\n",
       "      <td>0.104837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>43.127504</td>\n",
       "      <td>0.073523</td>\n",
       "      <td>-4.660363</td>\n",
       "      <td>-1.125556</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>54</td>\n",
       "      <td>-2.512137</td>\n",
       "      <td>-1.266368</td>\n",
       "      <td>-2.389360</td>\n",
       "      <td>-1.055698</td>\n",
       "      <td>-9.079592</td>\n",
       "      <td>-1.054603</td>\n",
       "      <td>1.357779</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>3.125269</td>\n",
       "      <td>0.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>34.736308</td>\n",
       "      <td>0.042351</td>\n",
       "      <td>-4.854120</td>\n",
       "      <td>-0.776878</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>55</td>\n",
       "      <td>-3.435222</td>\n",
       "      <td>-0.872690</td>\n",
       "      <td>-2.595169</td>\n",
       "      <td>-0.748760</td>\n",
       "      <td>-8.531968</td>\n",
       "      <td>-0.709184</td>\n",
       "      <td>4.545600</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>2.623147</td>\n",
       "      <td>0.069649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>14.218470</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>-5.450553</td>\n",
       "      <td>-1.620568</td>\n",
       "      <td>0.2</td>\n",
       "      <td>huber</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.2, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>56</td>\n",
       "      <td>-2.311396</td>\n",
       "      <td>-1.811218</td>\n",
       "      <td>-1.895459</td>\n",
       "      <td>-1.557541</td>\n",
       "      <td>-12.144804</td>\n",
       "      <td>-1.492946</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>4.736595</td>\n",
       "      <td>0.137365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>38.272984</td>\n",
       "      <td>0.089132</td>\n",
       "      <td>-5.478130</td>\n",
       "      <td>-0.890600</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>7</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>57</td>\n",
       "      <td>-3.338357</td>\n",
       "      <td>-0.995152</td>\n",
       "      <td>-3.055146</td>\n",
       "      <td>-0.855421</td>\n",
       "      <td>-10.040886</td>\n",
       "      <td>-0.821227</td>\n",
       "      <td>0.691896</td>\n",
       "      <td>0.006451</td>\n",
       "      <td>3.228427</td>\n",
       "      <td>0.075236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12.153365</td>\n",
       "      <td>0.061501</td>\n",
       "      <td>-5.622261</td>\n",
       "      <td>-1.716945</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>58</td>\n",
       "      <td>-2.832166</td>\n",
       "      <td>-1.962479</td>\n",
       "      <td>-2.433118</td>\n",
       "      <td>-1.635050</td>\n",
       "      <td>-11.601500</td>\n",
       "      <td>-1.553308</td>\n",
       "      <td>0.083211</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>4.231097</td>\n",
       "      <td>0.176796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>27.165137</td>\n",
       "      <td>0.075773</td>\n",
       "      <td>-6.196270</td>\n",
       "      <td>-1.276964</td>\n",
       "      <td>0.5</td>\n",
       "      <td>huber</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'huber', 'max_d...</td>\n",
       "      <td>59</td>\n",
       "      <td>-2.803826</td>\n",
       "      <td>-1.439064</td>\n",
       "      <td>-4.120530</td>\n",
       "      <td>-1.217429</td>\n",
       "      <td>-11.664453</td>\n",
       "      <td>-1.174398</td>\n",
       "      <td>0.453586</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>3.903776</td>\n",
       "      <td>0.115960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>23.575107</td>\n",
       "      <td>0.072735</td>\n",
       "      <td>-6.425318</td>\n",
       "      <td>-1.406107</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ls</td>\n",
       "      <td>5</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'ls', 'max_dept...</td>\n",
       "      <td>60</td>\n",
       "      <td>-3.287442</td>\n",
       "      <td>-1.605907</td>\n",
       "      <td>-5.093779</td>\n",
       "      <td>-1.331759</td>\n",
       "      <td>-10.894732</td>\n",
       "      <td>-1.280655</td>\n",
       "      <td>0.329811</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>3.245249</td>\n",
       "      <td>0.142812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "16      12.408733         0.042510        -1.727872         -1.621358   \n",
       "17      26.386224         0.068124        -1.731935         -1.482099   \n",
       "21      26.615493         0.068937        -1.791853         -1.558861   \n",
       "20      13.447075         0.051025        -1.803263         -1.674368   \n",
       "18      43.138781         0.077171        -1.814402         -1.307753   \n",
       "19      71.895877         0.086622        -1.841387         -1.099265   \n",
       "28      12.452810         0.058001        -1.842384         -1.583146   \n",
       "6       42.987909         0.078316        -1.859638         -1.696569   \n",
       "5       26.389244         0.060126        -1.861144         -1.748419   \n",
       "10      47.258625         0.094043        -1.897742         -1.749878   \n",
       "7       77.393106         0.096048        -1.905636         -1.584666   \n",
       "9       27.154617         0.062160        -1.929007         -1.814861   \n",
       "30      37.431016         0.071517        -1.929638         -1.253468   \n",
       "11      95.674379         0.104283        -1.930600         -1.664430   \n",
       "4       13.981733         0.047761        -1.943120         -1.822246   \n",
       "13      22.851825         0.055389        -1.954359         -1.621262   \n",
       "22      46.944064         0.089022        -1.961946         -1.408171   \n",
       "31      58.188299         0.081250        -1.970898         -1.042686   \n",
       "12      11.426562         0.054537        -1.979528         -1.768113   \n",
       "29      24.336335         0.072711        -1.983091         -1.420266   \n",
       "32      13.205518         0.052211        -1.991937         -1.639996   \n",
       "8       14.586561         0.061674        -2.032009         -1.962405   \n",
       "23      87.774307         0.105713        -2.074851         -1.148613   \n",
       "15      76.752856         0.087040        -2.190242         -1.232725   \n",
       "40      12.498520         0.057985        -2.269567         -1.541942   \n",
       "42      34.068051         0.084313        -2.354884         -1.202369   \n",
       "2       48.844235         0.088574        -2.460232         -2.324003   \n",
       "24      11.617735         0.058882        -2.474042         -1.728981   \n",
       "3       87.963250         0.116644        -2.482616         -2.241182   \n",
       "52      13.155516         0.056368        -2.493272         -1.505715   \n",
       "1       27.951357         0.064363        -2.496180         -2.407415   \n",
       "14      38.626225         0.064243        -2.511971         -1.476421   \n",
       "35      75.469110         0.095184        -2.528460         -0.970846   \n",
       "43      52.788463         0.074381        -2.530191         -1.002044   \n",
       "41      22.802124         0.066880        -2.553307         -1.379776   \n",
       "27      69.821680         0.085676        -2.744141         -1.057491   \n",
       "0       14.609785         0.081833        -2.763489         -2.647035   \n",
       "53      22.651472         0.077128        -2.850759         -1.333952   \n",
       "47      72.188706         0.079550        -3.122529         -0.720488   \n",
       "39      68.400009         0.094516        -3.141489         -0.795040   \n",
       "37      22.003787         0.061957        -3.181125         -1.526304   \n",
       "34      42.924056         0.070800        -3.306884         -1.314360   \n",
       "26      37.289499         0.072119        -3.337655         -1.371297   \n",
       "36      11.609967         0.051833        -3.381959         -1.710943   \n",
       "54      34.392599         0.091458        -3.533821         -1.119011   \n",
       "55      49.971275         0.078717        -3.750595         -0.913287   \n",
       "45      26.702921         0.063135        -3.822597         -1.427185   \n",
       "59      38.798088         0.033242        -4.213310         -0.309297   \n",
       "25      22.992695         0.053828        -4.309420         -1.574860   \n",
       "51      71.651513         0.099682        -4.459799         -0.364875   \n",
       "33      25.462525         0.067384        -4.502297         -1.510451   \n",
       "56      15.667155         0.068147        -4.560960         -1.584122   \n",
       "38      36.878244         0.076813        -4.562560         -1.228418   \n",
       "46      43.127504         0.073523        -4.660363         -1.125556   \n",
       "58      34.736308         0.042351        -4.854120         -0.776878   \n",
       "44      14.218470         0.063338        -5.450553         -1.620568   \n",
       "50      38.272984         0.089132        -5.478130         -0.890600   \n",
       "48      12.153365         0.061501        -5.622261         -1.716945   \n",
       "57      27.165137         0.075773        -6.196270         -1.276964   \n",
       "49      23.575107         0.072735        -6.425318         -1.406107   \n",
       "\n",
       "   param_learning_rate param_loss param_max_depth  \\\n",
       "16                0.05        lad               3   \n",
       "17                0.05        lad               5   \n",
       "21                0.05      huber               5   \n",
       "20                0.05      huber               3   \n",
       "18                0.05        lad               7   \n",
       "19                0.05        lad               9   \n",
       "28                 0.1        lad               3   \n",
       "6                 0.01        lad               7   \n",
       "5                 0.01        lad               5   \n",
       "10                0.01      huber               7   \n",
       "7                 0.01        lad               9   \n",
       "9                 0.01      huber               5   \n",
       "30                 0.1        lad               7   \n",
       "11                0.01      huber               9   \n",
       "4                 0.01        lad               3   \n",
       "13                0.05         ls               5   \n",
       "22                0.05      huber               7   \n",
       "31                 0.1        lad               9   \n",
       "12                0.05         ls               3   \n",
       "29                 0.1        lad               5   \n",
       "32                 0.1      huber               3   \n",
       "8                 0.01      huber               3   \n",
       "23                0.05      huber               9   \n",
       "15                0.05         ls               9   \n",
       "40                 0.2        lad               3   \n",
       "42                 0.2        lad               7   \n",
       "2                 0.01         ls               7   \n",
       "24                 0.1         ls               3   \n",
       "3                 0.01         ls               9   \n",
       "52                 0.5        lad               3   \n",
       "1                 0.01         ls               5   \n",
       "14                0.05         ls               7   \n",
       "35                 0.1      huber               9   \n",
       "43                 0.2        lad               9   \n",
       "41                 0.2        lad               5   \n",
       "27                 0.1         ls               9   \n",
       "0                 0.01         ls               3   \n",
       "53                 0.5        lad               5   \n",
       "47                 0.2      huber               9   \n",
       "39                 0.2         ls               9   \n",
       "37                 0.2         ls               5   \n",
       "34                 0.1      huber               7   \n",
       "26                 0.1         ls               7   \n",
       "36                 0.2         ls               3   \n",
       "54                 0.5        lad               7   \n",
       "55                 0.5        lad               9   \n",
       "45                 0.2      huber               5   \n",
       "59                 0.5      huber               9   \n",
       "25                 0.1         ls               5   \n",
       "51                 0.5         ls               9   \n",
       "33                 0.1      huber               5   \n",
       "56                 0.5      huber               3   \n",
       "38                 0.2         ls               7   \n",
       "46                 0.2      huber               7   \n",
       "58                 0.5      huber               7   \n",
       "44                 0.2      huber               3   \n",
       "50                 0.5         ls               7   \n",
       "48                 0.5         ls               3   \n",
       "57                 0.5      huber               5   \n",
       "49                 0.5         ls               5   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "16  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                1   \n",
       "17  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                2   \n",
       "21  {'learning_rate': 0.05, 'loss': 'huber', 'max_...                3   \n",
       "20  {'learning_rate': 0.05, 'loss': 'huber', 'max_...                4   \n",
       "18  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                5   \n",
       "19  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                6   \n",
       "28  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...                7   \n",
       "6   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...                8   \n",
       "5   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...                9   \n",
       "10  {'learning_rate': 0.01, 'loss': 'huber', 'max_...               10   \n",
       "7   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...               11   \n",
       "9   {'learning_rate': 0.01, 'loss': 'huber', 'max_...               12   \n",
       "30  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               13   \n",
       "11  {'learning_rate': 0.01, 'loss': 'huber', 'max_...               14   \n",
       "4   {'learning_rate': 0.01, 'loss': 'lad', 'max_de...               15   \n",
       "13  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               16   \n",
       "22  {'learning_rate': 0.05, 'loss': 'huber', 'max_...               17   \n",
       "31  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               18   \n",
       "12  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               19   \n",
       "29  {'learning_rate': 0.1, 'loss': 'lad', 'max_dep...               20   \n",
       "32  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               21   \n",
       "8   {'learning_rate': 0.01, 'loss': 'huber', 'max_...               22   \n",
       "23  {'learning_rate': 0.05, 'loss': 'huber', 'max_...               23   \n",
       "15  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               24   \n",
       "40  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               25   \n",
       "42  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               26   \n",
       "2   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               27   \n",
       "24  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               28   \n",
       "3   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               29   \n",
       "52  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               30   \n",
       "1   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               31   \n",
       "14  {'learning_rate': 0.05, 'loss': 'ls', 'max_dep...               32   \n",
       "35  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               33   \n",
       "43  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               34   \n",
       "41  {'learning_rate': 0.2, 'loss': 'lad', 'max_dep...               35   \n",
       "27  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               36   \n",
       "0   {'learning_rate': 0.01, 'loss': 'ls', 'max_dep...               37   \n",
       "53  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               38   \n",
       "47  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               39   \n",
       "39  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               40   \n",
       "37  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               41   \n",
       "34  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               42   \n",
       "26  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               43   \n",
       "36  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               44   \n",
       "54  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               45   \n",
       "55  {'learning_rate': 0.5, 'loss': 'lad', 'max_dep...               46   \n",
       "45  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               47   \n",
       "59  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               48   \n",
       "25  {'learning_rate': 0.1, 'loss': 'ls', 'max_dept...               49   \n",
       "51  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               50   \n",
       "33  {'learning_rate': 0.1, 'loss': 'huber', 'max_d...               51   \n",
       "56  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               52   \n",
       "38  {'learning_rate': 0.2, 'loss': 'ls', 'max_dept...               53   \n",
       "46  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               54   \n",
       "58  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               55   \n",
       "44  {'learning_rate': 0.2, 'loss': 'huber', 'max_d...               56   \n",
       "50  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               57   \n",
       "48  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               58   \n",
       "57  {'learning_rate': 0.5, 'loss': 'huber', 'max_d...               59   \n",
       "49  {'learning_rate': 0.5, 'loss': 'ls', 'max_dept...               60   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "16          -1.470123           -1.842604          -1.768484   \n",
       "17          -1.513285           -1.651843          -1.756297   \n",
       "21          -1.524351           -1.743231          -1.853912   \n",
       "20          -1.590906           -1.871494          -1.803082   \n",
       "18          -1.730167           -1.446984          -1.748325   \n",
       "19          -1.801479           -1.209665          -1.779027   \n",
       "28          -1.462753           -1.759570          -1.799038   \n",
       "6           -1.789523           -1.886022          -1.826049   \n",
       "5           -1.841710           -1.941143          -1.811980   \n",
       "10          -1.814639           -1.954869          -1.891613   \n",
       "7           -1.825757           -1.758809          -1.872378   \n",
       "9           -1.901725           -2.015583          -1.909856   \n",
       "30          -1.932853           -1.389050          -1.781885   \n",
       "11          -1.831218           -1.868343          -1.899867   \n",
       "4           -2.037378           -2.049960          -1.817407   \n",
       "13          -1.617713           -1.819882          -1.878392   \n",
       "22          -1.948573           -1.572034          -1.808551   \n",
       "31          -2.042239           -1.159061          -1.833949   \n",
       "12          -1.661228           -1.990412          -1.941462   \n",
       "29          -2.012590           -1.580498          -1.763511   \n",
       "32          -1.775052           -1.839935          -1.812237   \n",
       "8           -2.178538           -2.203643          -1.909795   \n",
       "23          -2.147608           -1.276495          -1.886568   \n",
       "15          -2.063649           -1.370648          -1.958671   \n",
       "40          -1.579116           -1.714248          -1.789248   \n",
       "42          -2.878401           -1.312147          -1.856960   \n",
       "2           -2.653820           -2.689870          -2.331474   \n",
       "24          -1.647166           -1.938385          -2.000044   \n",
       "3           -2.643710           -2.590111          -2.346420   \n",
       "52          -2.864763           -1.694434          -1.930032   \n",
       "1           -2.780718           -2.770946          -2.362601   \n",
       "14          -1.859132           -1.656558          -1.948277   \n",
       "35          -2.656695           -1.076976          -2.069040   \n",
       "43          -2.131337           -1.115096          -1.869918   \n",
       "41          -2.004566           -1.536712          -1.843151   \n",
       "27          -2.304364           -1.166990          -2.191175   \n",
       "0           -3.235000           -3.015445          -2.533918   \n",
       "53          -2.217088           -1.526027          -1.922535   \n",
       "47          -2.630357           -0.777981          -2.218549   \n",
       "39          -3.006337           -0.882903          -2.361843   \n",
       "37          -2.595424           -1.728139          -2.305569   \n",
       "34          -2.436873           -1.454265          -1.895583   \n",
       "26          -2.548415           -1.525439          -2.017653   \n",
       "36          -2.202419           -1.933081          -1.983976   \n",
       "54          -2.984584           -1.252455          -2.236219   \n",
       "55          -2.130313           -1.015405          -2.233688   \n",
       "45          -2.484309           -1.586996          -1.998976   \n",
       "59          -3.023806           -0.325901          -3.148984   \n",
       "25          -2.340784           -1.774491          -2.043054   \n",
       "51          -3.546979           -0.395417          -3.903283   \n",
       "33          -2.159830           -1.690819          -1.962968   \n",
       "56          -4.294324           -1.768909          -2.001934   \n",
       "38          -2.741141           -1.376624          -2.266411   \n",
       "46          -2.512137           -1.266368          -2.389360   \n",
       "58          -3.435222           -0.872690          -2.595169   \n",
       "44          -2.311396           -1.811218          -1.895459   \n",
       "50          -3.338357           -0.995152          -3.055146   \n",
       "48          -2.832166           -1.962479          -2.433118   \n",
       "57          -2.803826           -1.439064          -4.120530   \n",
       "49          -3.287442           -1.605907          -5.093779   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "16           -1.551389          -1.945008           -1.470083      0.562040   \n",
       "17           -1.430296          -1.926222           -1.364159      0.340439   \n",
       "21           -1.502879          -1.997297           -1.430474      0.381171   \n",
       "20           -1.607305          -2.015801           -1.544305      0.313712   \n",
       "18           -1.257051          -1.964714           -1.219225      0.564184   \n",
       "19           -1.044392          -1.943655           -1.043736      0.833252   \n",
       "28           -1.531443          -2.265361           -1.458426      0.168182   \n",
       "6            -1.626721          -1.963342           -1.576964      1.424292   \n",
       "5            -1.675511          -1.929742           -1.628602      0.691079   \n",
       "10           -1.671992          -1.986974           -1.622775      0.847066   \n",
       "7            -1.508971          -2.018774           -1.486219      1.495793   \n",
       "9            -1.742419          -1.975440           -1.686583      0.505933   \n",
       "30           -1.195953          -2.074177           -1.175401      0.363565   \n",
       "11           -1.582423          -2.060716           -1.542524      1.804403   \n",
       "4            -1.703278          -1.974575           -1.713500      0.151283   \n",
       "13           -1.554622          -2.366973           -1.489281      0.601397   \n",
       "22           -1.349839          -2.128712           -1.302641      0.582564   \n",
       "31           -0.992898          -2.036506           -0.976100      0.412954   \n",
       "12           -1.699707          -2.335895           -1.614222      0.407961   \n",
       "29           -1.369703          -2.173171           -1.310598      0.468893   \n",
       "32           -1.563534          -2.388522           -1.516520      0.182999   \n",
       "8            -1.864175          -2.007694           -1.819397      0.050646   \n",
       "23           -1.082055          -2.190377           -1.087287      1.572514   \n",
       "15           -1.167216          -2.548407           -1.160312      0.985835   \n",
       "40           -1.489417          -3.440339           -1.422161      0.213543   \n",
       "42           -1.162457          -2.329291           -1.132502      0.816304   \n",
       "2            -2.199878          -2.395401           -2.082263      1.860523   \n",
       "24           -1.657973          -3.774918           -1.590583      0.160408   \n",
       "3            -2.115677          -2.457720           -2.017758      1.874916   \n",
       "52           -1.430251          -2.685020           -1.392460      0.068936   \n",
       "1            -2.293836          -2.345221           -2.157461      0.334876   \n",
       "14           -1.409389          -3.728505           -1.363316      1.064254   \n",
       "35           -0.907608          -2.859644           -0.927955      1.775050   \n",
       "43           -0.967361          -3.589319           -0.923676      1.082908   \n",
       "41           -1.319659          -3.812204           -1.282957      0.315904   \n",
       "27           -1.009453          -3.736884           -0.996029      0.458487   \n",
       "0            -2.528554          -2.521550           -2.397105      0.221060   \n",
       "53           -1.246456          -4.412653           -1.229374      0.184523   \n",
       "47           -0.685091          -4.518682           -0.698393      1.868461   \n",
       "39           -0.744874          -4.056287           -0.757343      0.508664   \n",
       "37           -1.457869          -4.642382           -1.392903      0.145569   \n",
       "34           -1.251570          -5.588196           -1.237246      1.595740   \n",
       "26           -1.307659          -5.446897           -1.280793      1.150061   \n",
       "36           -1.626177          -5.959483           -1.573571      0.201665   \n",
       "54           -1.080975          -5.380660           -1.023601      0.587914   \n",
       "55           -0.910113          -6.887783           -0.814342      0.785924   \n",
       "45           -1.372545          -6.984506           -1.322013      0.986663   \n",
       "59           -0.270629          -6.467139           -0.331361      2.489530   \n",
       "25           -1.496368          -8.544422           -1.453721      0.511447   \n",
       "51           -0.362912          -5.929134           -0.336296      2.258859   \n",
       "33           -1.440226          -9.384093           -1.400309      0.201606   \n",
       "56           -1.523606          -7.386621           -1.459853      0.096616   \n",
       "38           -1.157835          -8.680128           -1.150794      1.089255   \n",
       "46           -1.055698          -9.079592           -1.054603      1.357779   \n",
       "58           -0.748760          -8.531968           -0.709184      4.545600   \n",
       "44           -1.557541         -12.144804           -1.492946      0.459823   \n",
       "50           -0.855421         -10.040886           -0.821227      0.691896   \n",
       "48           -1.635050         -11.601500           -1.553308      0.083211   \n",
       "57           -1.217429         -11.664453           -1.174398      0.453586   \n",
       "49           -1.331759         -10.894732           -1.280655      0.329811   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "16        0.003591        0.195986         0.159926  \n",
       "17        0.007063        0.169459         0.123026  \n",
       "21        0.003982        0.198003         0.133678  \n",
       "20        0.003979        0.173462         0.141742  \n",
       "18        0.003952        0.106545         0.099655  \n",
       "19        0.002965        0.072893         0.078066  \n",
       "28        0.002616        0.329093         0.128262  \n",
       "6         0.006486        0.074831         0.135495  \n",
       "5         0.002123        0.050001         0.137616  \n",
       "10        0.011767        0.070489         0.146336  \n",
       "7         0.012198        0.082233         0.123488  \n",
       "9         0.005135        0.033001         0.143750  \n",
       "30        0.005707        0.119349         0.096237  \n",
       "11        0.011514        0.096179         0.145106  \n",
       "4         0.004501        0.092516         0.161072  \n",
       "13        0.007060        0.310565         0.142956  \n",
       "22        0.010738        0.131047         0.117460  \n",
       "31        0.003512        0.096866         0.082575  \n",
       "12        0.001715        0.276744         0.161016  \n",
       "29        0.003010        0.168539         0.115842  \n",
       "32        0.005012        0.280838         0.142675  \n",
       "8         0.010459        0.111053         0.171558  \n",
       "23        0.009311        0.134277         0.090452  \n",
       "15        0.011798        0.256861         0.097567  \n",
       "40        0.006084        0.832293         0.124894  \n",
       "42        0.005109        0.417394         0.078582  \n",
       "2         0.006081        0.139354         0.263125  \n",
       "24        0.013704        0.931070         0.150606  \n",
       "3         0.019472        0.122638         0.249947  \n",
       "52        0.004754        0.404974         0.134334  \n",
       "1         0.001329        0.201324         0.263016  \n",
       "14        0.007902        0.860989         0.128757  \n",
       "35        0.007215        0.335258         0.075504  \n",
       "43        0.006827        0.756482         0.081905  \n",
       "41        0.006820        0.892610         0.111977  \n",
       "27        0.006745        0.703495         0.077622  \n",
       "0         0.017211        0.333447         0.265975  \n",
       "53        0.011255        1.110953         0.135996  \n",
       "47        0.004669        1.001442         0.041014  \n",
       "39        0.014728        0.698324         0.062337  \n",
       "37        0.010590        1.040018         0.145163  \n",
       "34        0.004000        1.628197         0.099100  \n",
       "26        0.002122        1.507117         0.109545  \n",
       "36        0.003727        1.824765         0.158537  \n",
       "54        0.007148        1.341174         0.097223  \n",
       "55        0.009275        2.218728         0.082114  \n",
       "45        0.000674        2.244570         0.114871  \n",
       "59        0.001674        1.594517         0.027433  \n",
       "25        0.002862        2.997064         0.142230  \n",
       "51        0.007486        1.049110         0.024176  \n",
       "33        0.004532        3.452887         0.128576  \n",
       "56        0.002109        2.206360         0.133231  \n",
       "38        0.006218        2.918003         0.104837  \n",
       "46        0.000151        3.125269         0.099570  \n",
       "58        0.001748        2.623147         0.069649  \n",
       "44        0.002777        4.736595         0.137365  \n",
       "50        0.006451        3.228427         0.075236  \n",
       "48        0.008621        4.231097         0.176796  \n",
       "57        0.007811        3.903776         0.115960  \n",
       "49        0.005448        3.245249         0.142812  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 21 candidates, totalling 63 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['lad'], \n",
    "    'learning_rate': [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08],\n",
    "    'max_depth': [2, 3, 4],\n",
    "}\n",
    "rf = GradientBoostingRegressor(n_estimators=100)\n",
    "cv_search = GridSearchCV(rf, param_grid,  n_jobs=-1,\n",
    "                         scoring='neg_median_absolute_error', cv=3, \n",
    "                         verbose=1, error_score = -999 )\n",
    "cv_search = cv_search.fit(X=features, y=target)\n",
    "search_results = pd.DataFrame(cv_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26.222965</td>\n",
       "      <td>0.081922</td>\n",
       "      <td>-1.714980</td>\n",
       "      <td>-1.545492</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.424364</td>\n",
       "      <td>-1.733693</td>\n",
       "      <td>-1.774675</td>\n",
       "      <td>-1.483623</td>\n",
       "      <td>-1.945901</td>\n",
       "      <td>-1.419161</td>\n",
       "      <td>0.871684</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.217060</td>\n",
       "      <td>0.135655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.621830</td>\n",
       "      <td>0.085226</td>\n",
       "      <td>-1.722834</td>\n",
       "      <td>-1.568248</td>\n",
       "      <td>0.04</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.04, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.456667</td>\n",
       "      <td>-1.769218</td>\n",
       "      <td>-1.770633</td>\n",
       "      <td>-1.496067</td>\n",
       "      <td>-1.941201</td>\n",
       "      <td>-1.439458</td>\n",
       "      <td>0.507786</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.200677</td>\n",
       "      <td>0.143975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.580471</td>\n",
       "      <td>0.080684</td>\n",
       "      <td>-1.727872</td>\n",
       "      <td>-1.621358</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.470123</td>\n",
       "      <td>-1.842604</td>\n",
       "      <td>-1.768484</td>\n",
       "      <td>-1.551389</td>\n",
       "      <td>-1.945008</td>\n",
       "      <td>-1.470083</td>\n",
       "      <td>0.755685</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.195986</td>\n",
       "      <td>0.159926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.586580</td>\n",
       "      <td>0.080759</td>\n",
       "      <td>-1.728592</td>\n",
       "      <td>-1.590066</td>\n",
       "      <td>0.03</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.03, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.493710</td>\n",
       "      <td>-1.785464</td>\n",
       "      <td>-1.772486</td>\n",
       "      <td>-1.523460</td>\n",
       "      <td>-1.919579</td>\n",
       "      <td>-1.461274</td>\n",
       "      <td>0.279504</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.176609</td>\n",
       "      <td>0.140480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.677033</td>\n",
       "      <td>0.077537</td>\n",
       "      <td>-1.728822</td>\n",
       "      <td>-1.527976</td>\n",
       "      <td>0.06</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.06, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.451144</td>\n",
       "      <td>-1.710497</td>\n",
       "      <td>-1.764573</td>\n",
       "      <td>-1.462963</td>\n",
       "      <td>-1.970749</td>\n",
       "      <td>-1.410469</td>\n",
       "      <td>0.307821</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.213629</td>\n",
       "      <td>0.130829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.701888</td>\n",
       "      <td>0.072255</td>\n",
       "      <td>-1.736207</td>\n",
       "      <td>-1.591168</td>\n",
       "      <td>0.07</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.07, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.474864</td>\n",
       "      <td>-1.780908</td>\n",
       "      <td>-1.764739</td>\n",
       "      <td>-1.530380</td>\n",
       "      <td>-1.969019</td>\n",
       "      <td>-1.462217</td>\n",
       "      <td>0.346410</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.202744</td>\n",
       "      <td>0.137022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.252835</td>\n",
       "      <td>0.072834</td>\n",
       "      <td>-1.742898</td>\n",
       "      <td>-1.625550</td>\n",
       "      <td>0.04</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.04, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.525615</td>\n",
       "      <td>-1.828152</td>\n",
       "      <td>-1.771541</td>\n",
       "      <td>-1.561803</td>\n",
       "      <td>-1.931538</td>\n",
       "      <td>-1.486694</td>\n",
       "      <td>0.407993</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.166951</td>\n",
       "      <td>0.146506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16.971516</td>\n",
       "      <td>0.073036</td>\n",
       "      <td>-1.743265</td>\n",
       "      <td>-1.608167</td>\n",
       "      <td>0.06</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.06, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.476378</td>\n",
       "      <td>-1.799321</td>\n",
       "      <td>-1.787186</td>\n",
       "      <td>-1.547519</td>\n",
       "      <td>-1.966231</td>\n",
       "      <td>-1.477661</td>\n",
       "      <td>0.627890</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.202379</td>\n",
       "      <td>0.138142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.902358</td>\n",
       "      <td>0.042355</td>\n",
       "      <td>-1.749982</td>\n",
       "      <td>-1.587997</td>\n",
       "      <td>0.08</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.08, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.493245</td>\n",
       "      <td>-1.773996</td>\n",
       "      <td>-1.794928</td>\n",
       "      <td>-1.525015</td>\n",
       "      <td>-1.961772</td>\n",
       "      <td>-1.464980</td>\n",
       "      <td>0.844521</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.193898</td>\n",
       "      <td>0.133786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.232171</td>\n",
       "      <td>0.085287</td>\n",
       "      <td>-1.757774</td>\n",
       "      <td>-1.628529</td>\n",
       "      <td>0.02</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.02, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.592911</td>\n",
       "      <td>-1.819346</td>\n",
       "      <td>-1.757121</td>\n",
       "      <td>-1.559329</td>\n",
       "      <td>-1.923288</td>\n",
       "      <td>-1.506912</td>\n",
       "      <td>0.324322</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.134877</td>\n",
       "      <td>0.136614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.567458</td>\n",
       "      <td>0.073355</td>\n",
       "      <td>-1.768191</td>\n",
       "      <td>-1.641830</td>\n",
       "      <td>0.03</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.03, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.611881</td>\n",
       "      <td>-1.846406</td>\n",
       "      <td>-1.767962</td>\n",
       "      <td>-1.571181</td>\n",
       "      <td>-1.924731</td>\n",
       "      <td>-1.507902</td>\n",
       "      <td>0.562222</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.127720</td>\n",
       "      <td>0.146946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.258516</td>\n",
       "      <td>0.073995</td>\n",
       "      <td>-1.773957</td>\n",
       "      <td>-1.675481</td>\n",
       "      <td>0.02</td>\n",
       "      <td>lad</td>\n",
       "      <td>3</td>\n",
       "      <td>{'learning_rate': 0.02, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.666063</td>\n",
       "      <td>-1.855734</td>\n",
       "      <td>-1.775983</td>\n",
       "      <td>-1.615053</td>\n",
       "      <td>-1.879826</td>\n",
       "      <td>-1.555657</td>\n",
       "      <td>0.399092</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.087280</td>\n",
       "      <td>0.129744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.623743</td>\n",
       "      <td>0.069589</td>\n",
       "      <td>-1.796622</td>\n",
       "      <td>-1.697713</td>\n",
       "      <td>0.07</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.07, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.522963</td>\n",
       "      <td>-1.887996</td>\n",
       "      <td>-1.863643</td>\n",
       "      <td>-1.630955</td>\n",
       "      <td>-2.003261</td>\n",
       "      <td>-1.574187</td>\n",
       "      <td>0.099984</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.201726</td>\n",
       "      <td>0.136532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24.507593</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>-1.798472</td>\n",
       "      <td>-1.520260</td>\n",
       "      <td>0.07</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.07, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.441421</td>\n",
       "      <td>-1.695360</td>\n",
       "      <td>-1.771561</td>\n",
       "      <td>-1.460012</td>\n",
       "      <td>-2.182433</td>\n",
       "      <td>-1.405409</td>\n",
       "      <td>0.406034</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.303115</td>\n",
       "      <td>0.125805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.180390</td>\n",
       "      <td>0.070232</td>\n",
       "      <td>-1.802575</td>\n",
       "      <td>-1.703553</td>\n",
       "      <td>0.06</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.06, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.581781</td>\n",
       "      <td>-1.906570</td>\n",
       "      <td>-1.845997</td>\n",
       "      <td>-1.629055</td>\n",
       "      <td>-1.979946</td>\n",
       "      <td>-1.575036</td>\n",
       "      <td>0.181026</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.165425</td>\n",
       "      <td>0.145238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.427012</td>\n",
       "      <td>0.067663</td>\n",
       "      <td>-1.810709</td>\n",
       "      <td>-1.714313</td>\n",
       "      <td>0.05</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.05, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.611192</td>\n",
       "      <td>-1.913600</td>\n",
       "      <td>-1.837167</td>\n",
       "      <td>-1.644369</td>\n",
       "      <td>-1.983767</td>\n",
       "      <td>-1.584970</td>\n",
       "      <td>0.107796</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.153249</td>\n",
       "      <td>0.142988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.883051</td>\n",
       "      <td>0.071940</td>\n",
       "      <td>-1.811941</td>\n",
       "      <td>-1.724580</td>\n",
       "      <td>0.04</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.04, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.626720</td>\n",
       "      <td>-1.923428</td>\n",
       "      <td>-1.839473</td>\n",
       "      <td>-1.657196</td>\n",
       "      <td>-1.969629</td>\n",
       "      <td>-1.593117</td>\n",
       "      <td>0.200275</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.141339</td>\n",
       "      <td>0.143019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.727013</td>\n",
       "      <td>0.071781</td>\n",
       "      <td>-1.813985</td>\n",
       "      <td>-1.695559</td>\n",
       "      <td>0.08</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.08, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.577925</td>\n",
       "      <td>-1.903942</td>\n",
       "      <td>-1.841165</td>\n",
       "      <td>-1.614786</td>\n",
       "      <td>-2.022865</td>\n",
       "      <td>-1.567949</td>\n",
       "      <td>0.028427</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.182660</td>\n",
       "      <td>0.148585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.103025</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>-1.824805</td>\n",
       "      <td>-1.723467</td>\n",
       "      <td>0.03</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.03, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>19</td>\n",
       "      <td>-1.657132</td>\n",
       "      <td>-1.885860</td>\n",
       "      <td>-1.841747</td>\n",
       "      <td>-1.670182</td>\n",
       "      <td>-1.975537</td>\n",
       "      <td>-1.614359</td>\n",
       "      <td>0.169303</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.130539</td>\n",
       "      <td>0.117069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.946913</td>\n",
       "      <td>0.069393</td>\n",
       "      <td>-1.885381</td>\n",
       "      <td>-1.805237</td>\n",
       "      <td>0.02</td>\n",
       "      <td>lad</td>\n",
       "      <td>2</td>\n",
       "      <td>{'learning_rate': 0.02, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>20</td>\n",
       "      <td>-1.812453</td>\n",
       "      <td>-1.972199</td>\n",
       "      <td>-1.845193</td>\n",
       "      <td>-1.723600</td>\n",
       "      <td>-1.998497</td>\n",
       "      <td>-1.719910</td>\n",
       "      <td>0.071319</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.081094</td>\n",
       "      <td>0.118070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.925755</td>\n",
       "      <td>0.023674</td>\n",
       "      <td>-2.182024</td>\n",
       "      <td>-1.508101</td>\n",
       "      <td>0.08</td>\n",
       "      <td>lad</td>\n",
       "      <td>4</td>\n",
       "      <td>{'learning_rate': 0.08, 'loss': 'lad', 'max_de...</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.514147</td>\n",
       "      <td>-1.681077</td>\n",
       "      <td>-1.761326</td>\n",
       "      <td>-1.441467</td>\n",
       "      <td>-3.270599</td>\n",
       "      <td>-1.401761</td>\n",
       "      <td>1.157868</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.776325</td>\n",
       "      <td>0.123382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "11      26.222965         0.081922        -1.714980         -1.545492   \n",
       "8       27.621830         0.085226        -1.722834         -1.568248   \n",
       "10      18.580471         0.080684        -1.727872         -1.621358   \n",
       "5       26.586580         0.080759        -1.728592         -1.590066   \n",
       "14      24.677033         0.077537        -1.728822         -1.527976   \n",
       "16      16.701888         0.072255        -1.736207         -1.591168   \n",
       "7       18.252835         0.072834        -1.742898         -1.625550   \n",
       "13      16.971516         0.073036        -1.743265         -1.608167   \n",
       "19      15.902358         0.042355        -1.749982         -1.587997   \n",
       "2       24.232171         0.085287        -1.757774         -1.628529   \n",
       "4       16.567458         0.073355        -1.768191         -1.641830   \n",
       "1       16.258516         0.073995        -1.773957         -1.675481   \n",
       "15      10.623743         0.069589        -1.796622         -1.697713   \n",
       "17      24.507593         0.080539        -1.798472         -1.520260   \n",
       "12      11.180390         0.070232        -1.802575         -1.703553   \n",
       "9       11.427012         0.067663        -1.810709         -1.714313   \n",
       "6       10.883051         0.071940        -1.811941         -1.724580   \n",
       "18      10.727013         0.071781        -1.813985         -1.695559   \n",
       "3       10.103025         0.073507        -1.824805         -1.723467   \n",
       "0        9.946913         0.069393        -1.885381         -1.805237   \n",
       "20      13.925755         0.023674        -2.182024         -1.508101   \n",
       "\n",
       "   param_learning_rate param_loss param_max_depth  \\\n",
       "11                0.05        lad               4   \n",
       "8                 0.04        lad               4   \n",
       "10                0.05        lad               3   \n",
       "5                 0.03        lad               4   \n",
       "14                0.06        lad               4   \n",
       "16                0.07        lad               3   \n",
       "7                 0.04        lad               3   \n",
       "13                0.06        lad               3   \n",
       "19                0.08        lad               3   \n",
       "2                 0.02        lad               4   \n",
       "4                 0.03        lad               3   \n",
       "1                 0.02        lad               3   \n",
       "15                0.07        lad               2   \n",
       "17                0.07        lad               4   \n",
       "12                0.06        lad               2   \n",
       "9                 0.05        lad               2   \n",
       "6                 0.04        lad               2   \n",
       "18                0.08        lad               2   \n",
       "3                 0.03        lad               2   \n",
       "0                 0.02        lad               2   \n",
       "20                0.08        lad               4   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "11  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                1   \n",
       "8   {'learning_rate': 0.04, 'loss': 'lad', 'max_de...                2   \n",
       "10  {'learning_rate': 0.05, 'loss': 'lad', 'max_de...                3   \n",
       "5   {'learning_rate': 0.03, 'loss': 'lad', 'max_de...                4   \n",
       "14  {'learning_rate': 0.06, 'loss': 'lad', 'max_de...                5   \n",
       "16  {'learning_rate': 0.07, 'loss': 'lad', 'max_de...                6   \n",
       "7   {'learning_rate': 0.04, 'loss': 'lad', 'max_de...                7   \n",
       "13  {'learning_rate': 0.06, 'loss': 'lad', 'max_de...                8   \n",
       "19  {'learning_rate': 0.08, 'loss': 'lad', 'max_de...                9   \n",
       "2   {'learning_rate': 0.02, 'loss': 'lad', 'max_de...               10   \n",
       "4   {'learning_rate': 0.03, 'loss': 'lad', 'max_de...               11   \n",
       "1   {'learning_rate': 0.02, 'loss': 'lad', 'max_de...               12   \n",
       "15  {'learning_rate': 0.07, 'loss': 'lad', 'max_de...               13   \n",
       "17  {'learning_rate': 0.07, 'loss': 'lad', 'max_de...               14   \n",
       "12  {'learning_rate': 0.06, 'loss': 'lad', 'max_de...               15   \n",
       "9   {'learning_rate': 0.05, 'loss': 'lad', 'max_de...               16   \n",
       "6   {'learning_rate': 0.04, 'loss': 'lad', 'max_de...               17   \n",
       "18  {'learning_rate': 0.08, 'loss': 'lad', 'max_de...               18   \n",
       "3   {'learning_rate': 0.03, 'loss': 'lad', 'max_de...               19   \n",
       "0   {'learning_rate': 0.02, 'loss': 'lad', 'max_de...               20   \n",
       "20  {'learning_rate': 0.08, 'loss': 'lad', 'max_de...               21   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "11          -1.424364           -1.733693          -1.774675   \n",
       "8           -1.456667           -1.769218          -1.770633   \n",
       "10          -1.470123           -1.842604          -1.768484   \n",
       "5           -1.493710           -1.785464          -1.772486   \n",
       "14          -1.451144           -1.710497          -1.764573   \n",
       "16          -1.474864           -1.780908          -1.764739   \n",
       "7           -1.525615           -1.828152          -1.771541   \n",
       "13          -1.476378           -1.799321          -1.787186   \n",
       "19          -1.493245           -1.773996          -1.794928   \n",
       "2           -1.592911           -1.819346          -1.757121   \n",
       "4           -1.611881           -1.846406          -1.767962   \n",
       "1           -1.666063           -1.855734          -1.775983   \n",
       "15          -1.522963           -1.887996          -1.863643   \n",
       "17          -1.441421           -1.695360          -1.771561   \n",
       "12          -1.581781           -1.906570          -1.845997   \n",
       "9           -1.611192           -1.913600          -1.837167   \n",
       "6           -1.626720           -1.923428          -1.839473   \n",
       "18          -1.577925           -1.903942          -1.841165   \n",
       "3           -1.657132           -1.885860          -1.841747   \n",
       "0           -1.812453           -1.972199          -1.845193   \n",
       "20          -1.514147           -1.681077          -1.761326   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "11           -1.483623          -1.945901           -1.419161      0.871684   \n",
       "8            -1.496067          -1.941201           -1.439458      0.507786   \n",
       "10           -1.551389          -1.945008           -1.470083      0.755685   \n",
       "5            -1.523460          -1.919579           -1.461274      0.279504   \n",
       "14           -1.462963          -1.970749           -1.410469      0.307821   \n",
       "16           -1.530380          -1.969019           -1.462217      0.346410   \n",
       "7            -1.561803          -1.931538           -1.486694      0.407993   \n",
       "13           -1.547519          -1.966231           -1.477661      0.627890   \n",
       "19           -1.525015          -1.961772           -1.464980      0.844521   \n",
       "2            -1.559329          -1.923288           -1.506912      0.324322   \n",
       "4            -1.571181          -1.924731           -1.507902      0.562222   \n",
       "1            -1.615053          -1.879826           -1.555657      0.399092   \n",
       "15           -1.630955          -2.003261           -1.574187      0.099984   \n",
       "17           -1.460012          -2.182433           -1.405409      0.406034   \n",
       "12           -1.629055          -1.979946           -1.575036      0.181026   \n",
       "9            -1.644369          -1.983767           -1.584970      0.107796   \n",
       "6            -1.657196          -1.969629           -1.593117      0.200275   \n",
       "18           -1.614786          -2.022865           -1.567949      0.028427   \n",
       "3            -1.670182          -1.975537           -1.614359      0.169303   \n",
       "0            -1.723600          -1.998497           -1.719910      0.071319   \n",
       "20           -1.441467          -3.270599           -1.401761      1.157868   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "11        0.002927        0.217060         0.135655  \n",
       "8         0.006160        0.200677         0.143975  \n",
       "10        0.004594        0.195986         0.159926  \n",
       "5         0.005847        0.176609         0.140480  \n",
       "14        0.005424        0.213629         0.130829  \n",
       "16        0.001946        0.202744         0.137022  \n",
       "7         0.001710        0.166951         0.146506  \n",
       "13        0.003771        0.202379         0.138142  \n",
       "19        0.009739        0.193898         0.133786  \n",
       "2         0.004966        0.134877         0.136614  \n",
       "4         0.001229        0.127720         0.146946  \n",
       "1         0.002379        0.087280         0.129744  \n",
       "15        0.002092        0.201726         0.136532  \n",
       "17        0.007236        0.303115         0.125805  \n",
       "12        0.004299        0.165425         0.145238  \n",
       "9         0.002648        0.153249         0.142988  \n",
       "6         0.004695        0.141339         0.143019  \n",
       "18        0.002727        0.182660         0.148585  \n",
       "3         0.004322        0.130539         0.117069  \n",
       "0         0.005040        0.081094         0.118070  \n",
       "20        0.004299        0.776325         0.123382  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  2.3min remaining:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000]\n",
    "}\n",
    "rf = GradientBoostingRegressor(learning_rate=0.05, loss='lad', max_depth=4)\n",
    "cv_search = GridSearchCV(rf, param_grid,  n_jobs=-1,\n",
    "                         scoring='neg_median_absolute_error', cv=3, \n",
    "                         verbose=1, error_score = -999 )\n",
    "cv_search = cv_search.fit(X=features, y=target)\n",
    "search_results = pd.DataFrame(cv_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.574056</td>\n",
       "      <td>0.085801</td>\n",
       "      <td>-1.716892</td>\n",
       "      <td>-1.547717</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.419683</td>\n",
       "      <td>-1.741488</td>\n",
       "      <td>-1.773855</td>\n",
       "      <td>-1.483623</td>\n",
       "      <td>-1.957139</td>\n",
       "      <td>-1.418038</td>\n",
       "      <td>0.655538</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>0.139609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.221561</td>\n",
       "      <td>0.139947</td>\n",
       "      <td>-1.934812</td>\n",
       "      <td>-1.498780</td>\n",
       "      <td>200</td>\n",
       "      <td>{'n_estimators': 200}</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.586267</td>\n",
       "      <td>-1.668713</td>\n",
       "      <td>-1.756489</td>\n",
       "      <td>-1.440745</td>\n",
       "      <td>-2.461679</td>\n",
       "      <td>-1.386882</td>\n",
       "      <td>0.928375</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.378977</td>\n",
       "      <td>0.122156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.080503</td>\n",
       "      <td>0.206450</td>\n",
       "      <td>-2.396435</td>\n",
       "      <td>-1.460645</td>\n",
       "      <td>500</td>\n",
       "      <td>{'n_estimators': 500}</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.070807</td>\n",
       "      <td>-1.626476</td>\n",
       "      <td>-1.774152</td>\n",
       "      <td>-1.399377</td>\n",
       "      <td>-2.344347</td>\n",
       "      <td>-1.356082</td>\n",
       "      <td>5.620712</td>\n",
       "      <td>0.052460</td>\n",
       "      <td>0.530637</td>\n",
       "      <td>0.118585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107.921420</td>\n",
       "      <td>0.129704</td>\n",
       "      <td>-2.441714</td>\n",
       "      <td>-1.453522</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000}</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.762014</td>\n",
       "      <td>-1.606037</td>\n",
       "      <td>-1.747245</td>\n",
       "      <td>-1.406245</td>\n",
       "      <td>-2.815884</td>\n",
       "      <td>-1.348283</td>\n",
       "      <td>7.728034</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.491556</td>\n",
       "      <td>0.110410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      24.574056         0.085801        -1.716892         -1.547717   \n",
       "1      45.221561         0.139947        -1.934812         -1.498780   \n",
       "2      86.080503         0.206450        -2.396435         -1.460645   \n",
       "3     107.921420         0.129704        -2.441714         -1.453522   \n",
       "\n",
       "  param_n_estimators                  params  rank_test_score  \\\n",
       "0                100   {'n_estimators': 100}                1   \n",
       "1                200   {'n_estimators': 200}                2   \n",
       "2                500   {'n_estimators': 500}                3   \n",
       "3               1000  {'n_estimators': 1000}                4   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0          -1.419683           -1.741488          -1.773855   \n",
       "1          -1.586267           -1.668713          -1.756489   \n",
       "2          -3.070807           -1.626476          -1.774152   \n",
       "3          -2.762014           -1.606037          -1.747245   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0           -1.483623          -1.957139           -1.418038      0.655538   \n",
       "1           -1.440745          -2.461679           -1.386882      0.928375   \n",
       "2           -1.399377          -2.344347           -1.356082      5.620712   \n",
       "3           -1.406245          -2.815884           -1.348283      7.728034   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.005772        0.223082         0.139609  \n",
       "1        0.010524        0.378977         0.122156  \n",
       "2        0.052460        0.530637         0.118585  \n",
       "3        0.025758        0.491556         0.110410  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted trees can be tuned to 1.7 minutes median error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
